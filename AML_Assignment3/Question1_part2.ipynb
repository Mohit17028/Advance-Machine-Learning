{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "path = \"../AML_Assignment2/cifar/\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extractImagesAndLabels(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dicte = pickle.load(f,encoding='bytes')\n",
    "    images = dicte[b'data']\n",
    "    Matrix=[]\n",
    "    image_mean = np.mean(images)\n",
    "    for i in images:\n",
    "        ingle_img_reshaped = np.reshape(i,(3, 32,32))\n",
    "        ingle_img_reshaped = ingle_img_reshaped - image_mean\n",
    "        ingle_img_reshaped = ingle_img_reshaped/128\n",
    "#         ingle_img_reshaped = np.transpose(ingle_img_reshaped, (1,2,0))\n",
    "        #x=np.dot(ingle_img_reshaped[...,:3], [0.299, 0.587, 0.114])\n",
    "#         x= ingle_img_reshaped.flatten()\n",
    "        Matrix.append(ingle_img_reshaped)\n",
    "    #images = np.transpose(images, (1,2,0))\n",
    "   \n",
    "    Matrix = np.array(Matrix)\n",
    "    labels = dicte[b'labels']\n",
    "    labels = np.array(labels)\n",
    "    #print labels.shape\n",
    "    return Matrix, labels\n",
    " \n",
    "#     labels = dict['labels']\n",
    "\n",
    "def extractCategories(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dict = pickle.load(f,encoding='bytes')\n",
    "    return dict[b'label_names']\n",
    "\n",
    "def saveCifarImage(array, path, file):\n",
    "    # array is 3x32x32. cv2 needs 32x32x3\n",
    "    array = array.asnumpy().transpose(1,2,0)\n",
    "    # array is RGB. cv2 needs BGR\n",
    "    array = cv2.cvtColor(array, cv2.COLOR_RGB2BGR)\n",
    "    # save to PNG file\n",
    "    return cv2.imwrite(path+file+\".png\", array)\n",
    "\n",
    "\n",
    "Train_data, Train_labels = extractImagesAndLabels(path, \"data_batch_1\")\n",
    "Train_data_2, Train_labels_2 = extractImagesAndLabels(path,\"data_batch_2\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_4, Train_labels_4 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_5, Train_labels_5 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_t = np.concatenate((Train_data, Train_data_2,Train_data_3,Train_data_4,Train_data_5), axis=0)\n",
    "Train_labels_t = np.concatenate((Train_labels,Train_labels_2,Train_labels_3,Train_labels_4,Train_labels_5), axis=0)\n",
    "print (Train_data_t.shape)\n",
    "Test_data, Test_labels = extractImagesAndLabels(path, \"test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Linear):\n",
    "        size = m.weight.size()\n",
    "        fan_out = size[0] # number of rows\n",
    "        fan_in = size[1] # number of columns\n",
    "        variance = np.sqrt(2.0/(fan_in + fan_out))\n",
    "        m.weight.data.normal_(0.0, variance)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0),            \n",
    "                    nn.PReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "#         torch.nn.init.xavier_uniform(self.layer1.weight)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "                    nn.PReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                       nn.Conv2d(32,64,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU()\n",
    "                    )\n",
    "        self.layer4 = nn.Sequential(\n",
    "                       nn.Conv2d(64,64,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU())\n",
    "        self.fc1 = nn.Linear(2304, 500)\n",
    "        self.pre = nn.PReLU()\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "        self.pre2 = nn.PReLU()\n",
    "        self.soft = nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "#         out = out.view(out\n",
    "#         out = out.reshape()\n",
    "#         print(out.size())\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.size())\n",
    "        out = self.pre(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.pre2(out)\n",
    "#         print(out.size())\n",
    "        out = self.soft(out)\n",
    "#         print(out.size())\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0),            \n",
    "                    nn.PReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "#         torch.nn.init.xavier_uniform(self.layer1.weight)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n",
    "                    nn.PReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                       nn.Conv2d(32,64,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU()\n",
    "                    )\n",
    "        self.layer4 = nn.Sequential(\n",
    "                       nn.Conv2d(64,64,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "                       nn.Conv2d(64,128,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "                       nn.Conv2d(128,128,kernel_size=3, stride=1, padding=0),\n",
    "                        nn.PReLU())\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.pre = nn.PReLU()\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.pre2 = nn.PReLU()\n",
    "        self.soft = nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "#         out = out.view(out\n",
    "#         out = out.reshape()\n",
    "#         print(out.size())\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.size())\n",
    "        out = self.pre(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.pre2(out)\n",
    "#         print(out.size())\n",
    "        out = self.soft(out)\n",
    "#         print(out.size())\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-8c8ca508cb04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTrain_data_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_data_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mTrain_labels_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_labels_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_data_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_labels_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "\n",
    "Train_data_t = torch.from_numpy(Train_data_t)\n",
    "Train_labels_t = torch.from_numpy(Train_labels_t)\n",
    "train = data_utils.TensorDataset(Train_data_t, Train_labels_t)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "model = ConvNet(num_classes).cuda()\n",
    "# model.apply(weight_init)\n",
    "# print(model)\n",
    "# exam = torch.FloatTensor(Train_data_t[0,:,:,:])\n",
    "# exam = exam.unsqueeze(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "num_epochs=100\n",
    "from torch.autograd import Variable\n",
    "total_step =len(Train_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/50000], Loss: 2.3026\n",
      "Epoch [1/100], Step [200/50000], Loss: 2.3022\n",
      "Epoch [1/100], Step [300/50000], Loss: 2.3026\n",
      "Epoch [1/100], Step [400/50000], Loss: 2.3025\n",
      "Epoch [1/100], Step [500/50000], Loss: 2.3018\n",
      "Epoch [1/100], Step [600/50000], Loss: 2.3032\n",
      "Epoch [1/100], Step [700/50000], Loss: 2.3027\n",
      "Epoch [1/100], Step [800/50000], Loss: 2.3033\n",
      "Epoch [1/100], Step [900/50000], Loss: 2.3020\n",
      "Epoch [1/100], Step [1000/50000], Loss: 2.3026\n",
      "Epoch [2/100], Step [100/50000], Loss: 2.3028\n",
      "Epoch [2/100], Step [200/50000], Loss: 2.3026\n",
      "Epoch [2/100], Step [300/50000], Loss: 2.3029\n",
      "Epoch [2/100], Step [400/50000], Loss: 2.3034\n",
      "Epoch [2/100], Step [500/50000], Loss: 2.3022\n",
      "Epoch [2/100], Step [600/50000], Loss: 2.3024\n",
      "Epoch [2/100], Step [700/50000], Loss: 2.3023\n",
      "Epoch [2/100], Step [800/50000], Loss: 2.3024\n",
      "Epoch [2/100], Step [900/50000], Loss: 2.3023\n",
      "Epoch [2/100], Step [1000/50000], Loss: 2.3026\n",
      "Epoch [3/100], Step [100/50000], Loss: 2.3026\n",
      "Epoch [3/100], Step [200/50000], Loss: 2.3026\n",
      "Epoch [3/100], Step [300/50000], Loss: 2.3016\n",
      "Epoch [3/100], Step [400/50000], Loss: 2.3024\n",
      "Epoch [3/100], Step [500/50000], Loss: 2.3021\n",
      "Epoch [3/100], Step [600/50000], Loss: 2.3022\n",
      "Epoch [3/100], Step [700/50000], Loss: 2.3023\n",
      "Epoch [3/100], Step [800/50000], Loss: 2.3015\n",
      "Epoch [3/100], Step [900/50000], Loss: 2.3024\n",
      "Epoch [3/100], Step [1000/50000], Loss: 2.3022\n",
      "Epoch [4/100], Step [100/50000], Loss: 2.3022\n",
      "Epoch [4/100], Step [200/50000], Loss: 2.3020\n",
      "Epoch [4/100], Step [300/50000], Loss: 2.3024\n",
      "Epoch [4/100], Step [400/50000], Loss: 2.3019\n",
      "Epoch [4/100], Step [500/50000], Loss: 2.3020\n",
      "Epoch [4/100], Step [600/50000], Loss: 2.3021\n",
      "Epoch [4/100], Step [700/50000], Loss: 2.3021\n",
      "Epoch [4/100], Step [800/50000], Loss: 2.3022\n",
      "Epoch [4/100], Step [900/50000], Loss: 2.3020\n",
      "Epoch [4/100], Step [1000/50000], Loss: 2.3022\n",
      "Epoch [5/100], Step [100/50000], Loss: 2.3012\n",
      "Epoch [5/100], Step [200/50000], Loss: 2.3015\n",
      "Epoch [5/100], Step [300/50000], Loss: 2.3010\n",
      "Epoch [5/100], Step [400/50000], Loss: 2.3021\n",
      "Epoch [5/100], Step [500/50000], Loss: 2.3007\n",
      "Epoch [5/100], Step [600/50000], Loss: 2.3009\n",
      "Epoch [5/100], Step [700/50000], Loss: 2.3009\n",
      "Epoch [5/100], Step [800/50000], Loss: 2.3008\n",
      "Epoch [5/100], Step [900/50000], Loss: 2.2988\n",
      "Epoch [5/100], Step [1000/50000], Loss: 2.3004\n",
      "Epoch [6/100], Step [100/50000], Loss: 2.3012\n",
      "Epoch [6/100], Step [200/50000], Loss: 2.3003\n",
      "Epoch [6/100], Step [300/50000], Loss: 2.2994\n",
      "Epoch [6/100], Step [400/50000], Loss: 2.2967\n",
      "Epoch [6/100], Step [500/50000], Loss: 2.3017\n",
      "Epoch [6/100], Step [600/50000], Loss: 2.2981\n",
      "Epoch [6/100], Step [700/50000], Loss: 2.2970\n",
      "Epoch [6/100], Step [800/50000], Loss: 2.2956\n",
      "Epoch [6/100], Step [900/50000], Loss: 2.3039\n",
      "Epoch [6/100], Step [1000/50000], Loss: 2.2867\n",
      "Epoch [7/100], Step [100/50000], Loss: 2.2928\n",
      "Epoch [7/100], Step [200/50000], Loss: 2.3038\n",
      "Epoch [7/100], Step [300/50000], Loss: 2.2843\n",
      "Epoch [7/100], Step [400/50000], Loss: 2.2729\n",
      "Epoch [7/100], Step [500/50000], Loss: 2.2668\n",
      "Epoch [7/100], Step [600/50000], Loss: 2.2881\n",
      "Epoch [7/100], Step [700/50000], Loss: 2.2835\n",
      "Epoch [7/100], Step [800/50000], Loss: 2.2831\n",
      "Epoch [7/100], Step [900/50000], Loss: 2.2730\n",
      "Epoch [7/100], Step [1000/50000], Loss: 2.2832\n",
      "Epoch [8/100], Step [100/50000], Loss: 2.2642\n",
      "Epoch [8/100], Step [200/50000], Loss: 2.2989\n",
      "Epoch [8/100], Step [300/50000], Loss: 2.2530\n",
      "Epoch [8/100], Step [400/50000], Loss: 2.2305\n",
      "Epoch [8/100], Step [500/50000], Loss: 2.2628\n",
      "Epoch [8/100], Step [600/50000], Loss: 2.2421\n",
      "Epoch [8/100], Step [700/50000], Loss: 2.2196\n",
      "Epoch [8/100], Step [800/50000], Loss: 2.2891\n",
      "Epoch [8/100], Step [900/50000], Loss: 2.2414\n",
      "Epoch [8/100], Step [1000/50000], Loss: 2.1872\n",
      "Epoch [9/100], Step [100/50000], Loss: 2.2523\n",
      "Epoch [9/100], Step [200/50000], Loss: 2.1834\n",
      "Epoch [9/100], Step [300/50000], Loss: 2.2560\n",
      "Epoch [9/100], Step [400/50000], Loss: 2.2565\n",
      "Epoch [9/100], Step [500/50000], Loss: 2.2227\n",
      "Epoch [9/100], Step [600/50000], Loss: 2.1987\n",
      "Epoch [9/100], Step [700/50000], Loss: 2.1214\n",
      "Epoch [9/100], Step [800/50000], Loss: 2.2427\n",
      "Epoch [9/100], Step [900/50000], Loss: 2.1927\n",
      "Epoch [9/100], Step [1000/50000], Loss: 2.0763\n",
      "Epoch [10/100], Step [100/50000], Loss: 2.1225\n",
      "Epoch [10/100], Step [200/50000], Loss: 2.1254\n",
      "Epoch [10/100], Step [300/50000], Loss: 2.1791\n",
      "Epoch [10/100], Step [400/50000], Loss: 2.1831\n",
      "Epoch [10/100], Step [500/50000], Loss: 2.2366\n",
      "Epoch [10/100], Step [600/50000], Loss: 2.1440\n",
      "Epoch [10/100], Step [700/50000], Loss: 2.1277\n",
      "Epoch [10/100], Step [800/50000], Loss: 2.1475\n",
      "Epoch [10/100], Step [900/50000], Loss: 2.1665\n",
      "Epoch [10/100], Step [1000/50000], Loss: 2.1210\n",
      "Epoch [11/100], Step [100/50000], Loss: 2.1248\n",
      "Epoch [11/100], Step [200/50000], Loss: 2.0918\n",
      "Epoch [11/100], Step [300/50000], Loss: 2.0637\n",
      "Epoch [11/100], Step [400/50000], Loss: 2.1076\n",
      "Epoch [11/100], Step [500/50000], Loss: 2.1701\n",
      "Epoch [11/100], Step [600/50000], Loss: 2.1602\n",
      "Epoch [11/100], Step [700/50000], Loss: 2.1582\n",
      "Epoch [11/100], Step [800/50000], Loss: 2.0194\n",
      "Epoch [11/100], Step [900/50000], Loss: 2.0904\n",
      "Epoch [11/100], Step [1000/50000], Loss: 2.0564\n",
      "Epoch [12/100], Step [100/50000], Loss: 2.0382\n",
      "Epoch [12/100], Step [200/50000], Loss: 2.1409\n",
      "Epoch [12/100], Step [300/50000], Loss: 2.1670\n",
      "Epoch [12/100], Step [400/50000], Loss: 2.0102\n",
      "Epoch [12/100], Step [500/50000], Loss: 2.1636\n",
      "Epoch [12/100], Step [600/50000], Loss: 2.0340\n",
      "Epoch [12/100], Step [700/50000], Loss: 2.1313\n",
      "Epoch [12/100], Step [800/50000], Loss: 2.1205\n",
      "Epoch [12/100], Step [900/50000], Loss: 2.0917\n",
      "Epoch [12/100], Step [1000/50000], Loss: 2.1134\n",
      "Epoch [13/100], Step [100/50000], Loss: 2.1193\n",
      "Epoch [13/100], Step [200/50000], Loss: 2.1551\n",
      "Epoch [13/100], Step [300/50000], Loss: 2.1442\n",
      "Epoch [13/100], Step [400/50000], Loss: 2.1825\n",
      "Epoch [13/100], Step [500/50000], Loss: 2.0963\n",
      "Epoch [13/100], Step [600/50000], Loss: 2.0045\n",
      "Epoch [13/100], Step [700/50000], Loss: 2.0491\n",
      "Epoch [13/100], Step [800/50000], Loss: 2.0157\n",
      "Epoch [13/100], Step [900/50000], Loss: 2.0294\n",
      "Epoch [13/100], Step [1000/50000], Loss: 2.0675\n",
      "Epoch [14/100], Step [100/50000], Loss: 2.0682\n",
      "Epoch [14/100], Step [200/50000], Loss: 2.1145\n",
      "Epoch [14/100], Step [300/50000], Loss: 2.1067\n",
      "Epoch [14/100], Step [400/50000], Loss: 2.0034\n",
      "Epoch [14/100], Step [500/50000], Loss: 2.0378\n",
      "Epoch [14/100], Step [600/50000], Loss: 2.0683\n",
      "Epoch [14/100], Step [700/50000], Loss: 2.1997\n",
      "Epoch [14/100], Step [800/50000], Loss: 2.1264\n",
      "Epoch [14/100], Step [900/50000], Loss: 2.0475\n",
      "Epoch [14/100], Step [1000/50000], Loss: 2.0684\n",
      "Epoch [15/100], Step [100/50000], Loss: 2.0736\n",
      "Epoch [15/100], Step [200/50000], Loss: 2.1357\n",
      "Epoch [15/100], Step [300/50000], Loss: 2.0539\n",
      "Epoch [15/100], Step [400/50000], Loss: 2.1193\n",
      "Epoch [15/100], Step [500/50000], Loss: 2.0186\n",
      "Epoch [15/100], Step [600/50000], Loss: 2.0170\n",
      "Epoch [15/100], Step [700/50000], Loss: 1.9437\n",
      "Epoch [15/100], Step [800/50000], Loss: 1.9292\n",
      "Epoch [15/100], Step [900/50000], Loss: 2.0649\n",
      "Epoch [15/100], Step [1000/50000], Loss: 2.0516\n",
      "Epoch [16/100], Step [100/50000], Loss: 2.1080\n",
      "Epoch [16/100], Step [200/50000], Loss: 2.0995\n",
      "Epoch [16/100], Step [300/50000], Loss: 2.1503\n",
      "Epoch [16/100], Step [400/50000], Loss: 2.0661\n",
      "Epoch [16/100], Step [500/50000], Loss: 2.0864\n",
      "Epoch [16/100], Step [600/50000], Loss: 1.9413\n",
      "Epoch [16/100], Step [700/50000], Loss: 1.9756\n",
      "Epoch [16/100], Step [800/50000], Loss: 1.9863\n",
      "Epoch [16/100], Step [900/50000], Loss: 2.0106\n",
      "Epoch [16/100], Step [1000/50000], Loss: 1.9077\n",
      "Epoch [17/100], Step [100/50000], Loss: 2.0430\n",
      "Epoch [17/100], Step [200/50000], Loss: 2.0203\n",
      "Epoch [17/100], Step [300/50000], Loss: 2.0829\n",
      "Epoch [17/100], Step [400/50000], Loss: 1.9534\n",
      "Epoch [17/100], Step [500/50000], Loss: 2.0719\n",
      "Epoch [17/100], Step [600/50000], Loss: 2.0109\n",
      "Epoch [17/100], Step [700/50000], Loss: 1.9908\n",
      "Epoch [17/100], Step [800/50000], Loss: 1.9763\n",
      "Epoch [17/100], Step [900/50000], Loss: 2.0198\n",
      "Epoch [17/100], Step [1000/50000], Loss: 2.0823\n",
      "Epoch [18/100], Step [100/50000], Loss: 2.0625\n",
      "Epoch [18/100], Step [200/50000], Loss: 2.1035\n",
      "Epoch [18/100], Step [300/50000], Loss: 2.0178\n",
      "Epoch [18/100], Step [400/50000], Loss: 2.0641\n",
      "Epoch [18/100], Step [500/50000], Loss: 2.0264\n",
      "Epoch [18/100], Step [600/50000], Loss: 2.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Step [700/50000], Loss: 2.0261\n",
      "Epoch [18/100], Step [800/50000], Loss: 2.0658\n",
      "Epoch [18/100], Step [900/50000], Loss: 1.9454\n",
      "Epoch [18/100], Step [1000/50000], Loss: 1.9527\n",
      "Epoch [19/100], Step [100/50000], Loss: 2.0351\n",
      "Epoch [19/100], Step [200/50000], Loss: 2.0606\n",
      "Epoch [19/100], Step [300/50000], Loss: 2.1119\n",
      "Epoch [19/100], Step [400/50000], Loss: 1.8834\n",
      "Epoch [19/100], Step [500/50000], Loss: 2.0928\n",
      "Epoch [19/100], Step [600/50000], Loss: 1.9142\n",
      "Epoch [19/100], Step [700/50000], Loss: 1.9628\n",
      "Epoch [19/100], Step [800/50000], Loss: 2.0209\n",
      "Epoch [19/100], Step [900/50000], Loss: 2.0317\n",
      "Epoch [19/100], Step [1000/50000], Loss: 2.0216\n",
      "Epoch [20/100], Step [100/50000], Loss: 2.0127\n",
      "Epoch [20/100], Step [200/50000], Loss: 1.9997\n",
      "Epoch [20/100], Step [300/50000], Loss: 1.9320\n",
      "Epoch [20/100], Step [400/50000], Loss: 1.9731\n",
      "Epoch [20/100], Step [500/50000], Loss: 1.9713\n",
      "Epoch [20/100], Step [600/50000], Loss: 2.0711\n",
      "Epoch [20/100], Step [700/50000], Loss: 1.8583\n",
      "Epoch [20/100], Step [800/50000], Loss: 2.1042\n",
      "Epoch [20/100], Step [900/50000], Loss: 2.0043\n",
      "Epoch [20/100], Step [1000/50000], Loss: 2.0067\n",
      "Epoch [21/100], Step [100/50000], Loss: 2.0449\n",
      "Epoch [21/100], Step [200/50000], Loss: 2.0898\n",
      "Epoch [21/100], Step [300/50000], Loss: 2.0591\n",
      "Epoch [21/100], Step [400/50000], Loss: 2.0113\n",
      "Epoch [21/100], Step [500/50000], Loss: 1.9652\n",
      "Epoch [21/100], Step [600/50000], Loss: 1.9348\n",
      "Epoch [21/100], Step [700/50000], Loss: 2.0418\n",
      "Epoch [21/100], Step [800/50000], Loss: 1.9170\n",
      "Epoch [21/100], Step [900/50000], Loss: 2.0152\n",
      "Epoch [21/100], Step [1000/50000], Loss: 2.0596\n",
      "Epoch [22/100], Step [100/50000], Loss: 2.1072\n",
      "Epoch [22/100], Step [200/50000], Loss: 2.0537\n",
      "Epoch [22/100], Step [300/50000], Loss: 1.8984\n",
      "Epoch [22/100], Step [400/50000], Loss: 1.8230\n",
      "Epoch [22/100], Step [500/50000], Loss: 1.9700\n",
      "Epoch [22/100], Step [600/50000], Loss: 2.0811\n",
      "Epoch [22/100], Step [700/50000], Loss: 2.0214\n",
      "Epoch [22/100], Step [800/50000], Loss: 2.0166\n",
      "Epoch [22/100], Step [900/50000], Loss: 1.9030\n",
      "Epoch [22/100], Step [1000/50000], Loss: 2.0421\n",
      "Epoch [23/100], Step [100/50000], Loss: 1.9460\n",
      "Epoch [23/100], Step [200/50000], Loss: 1.9063\n",
      "Epoch [23/100], Step [300/50000], Loss: 1.8910\n",
      "Epoch [23/100], Step [400/50000], Loss: 1.9819\n",
      "Epoch [23/100], Step [500/50000], Loss: 1.9639\n",
      "Epoch [23/100], Step [600/50000], Loss: 1.9989\n",
      "Epoch [23/100], Step [700/50000], Loss: 1.8346\n",
      "Epoch [23/100], Step [800/50000], Loss: 1.9695\n",
      "Epoch [23/100], Step [900/50000], Loss: 1.9189\n",
      "Epoch [23/100], Step [1000/50000], Loss: 1.9531\n",
      "Epoch [24/100], Step [100/50000], Loss: 1.9519\n",
      "Epoch [24/100], Step [200/50000], Loss: 2.0567\n",
      "Epoch [24/100], Step [300/50000], Loss: 1.9731\n",
      "Epoch [24/100], Step [400/50000], Loss: 1.9502\n",
      "Epoch [24/100], Step [500/50000], Loss: 1.8422\n",
      "Epoch [24/100], Step [600/50000], Loss: 2.0074\n",
      "Epoch [24/100], Step [700/50000], Loss: 2.0556\n",
      "Epoch [24/100], Step [800/50000], Loss: 1.9577\n",
      "Epoch [24/100], Step [900/50000], Loss: 1.9647\n",
      "Epoch [24/100], Step [1000/50000], Loss: 2.0423\n",
      "Epoch [25/100], Step [100/50000], Loss: 1.9286\n",
      "Epoch [25/100], Step [200/50000], Loss: 2.0043\n",
      "Epoch [25/100], Step [300/50000], Loss: 1.9019\n",
      "Epoch [25/100], Step [400/50000], Loss: 1.9674\n",
      "Epoch [25/100], Step [500/50000], Loss: 1.9144\n",
      "Epoch [25/100], Step [600/50000], Loss: 1.8834\n",
      "Epoch [25/100], Step [700/50000], Loss: 1.9468\n",
      "Epoch [25/100], Step [800/50000], Loss: 1.9651\n",
      "Epoch [25/100], Step [900/50000], Loss: 1.8468\n",
      "Epoch [25/100], Step [1000/50000], Loss: 1.8809\n",
      "Epoch [26/100], Step [100/50000], Loss: 1.8077\n",
      "Epoch [26/100], Step [200/50000], Loss: 2.0094\n",
      "Epoch [26/100], Step [300/50000], Loss: 1.8613\n",
      "Epoch [26/100], Step [400/50000], Loss: 1.9011\n",
      "Epoch [26/100], Step [500/50000], Loss: 1.8446\n",
      "Epoch [26/100], Step [600/50000], Loss: 1.9176\n",
      "Epoch [26/100], Step [700/50000], Loss: 1.8859\n",
      "Epoch [26/100], Step [800/50000], Loss: 2.0350\n",
      "Epoch [26/100], Step [900/50000], Loss: 1.8637\n",
      "Epoch [26/100], Step [1000/50000], Loss: 1.9053\n",
      "Epoch [27/100], Step [100/50000], Loss: 1.8531\n",
      "Epoch [27/100], Step [200/50000], Loss: 1.8136\n",
      "Epoch [27/100], Step [300/50000], Loss: 1.9497\n",
      "Epoch [27/100], Step [400/50000], Loss: 1.9501\n",
      "Epoch [27/100], Step [500/50000], Loss: 1.8185\n",
      "Epoch [27/100], Step [600/50000], Loss: 1.9773\n",
      "Epoch [27/100], Step [700/50000], Loss: 1.7795\n",
      "Epoch [27/100], Step [800/50000], Loss: 1.9157\n",
      "Epoch [27/100], Step [900/50000], Loss: 1.8894\n",
      "Epoch [27/100], Step [1000/50000], Loss: 1.8282\n",
      "Epoch [28/100], Step [100/50000], Loss: 1.9174\n",
      "Epoch [28/100], Step [200/50000], Loss: 1.9882\n",
      "Epoch [28/100], Step [300/50000], Loss: 1.8530\n",
      "Epoch [28/100], Step [400/50000], Loss: 1.8200\n",
      "Epoch [28/100], Step [500/50000], Loss: 1.8720\n",
      "Epoch [28/100], Step [600/50000], Loss: 1.8692\n",
      "Epoch [28/100], Step [700/50000], Loss: 1.7838\n",
      "Epoch [28/100], Step [800/50000], Loss: 1.9089\n",
      "Epoch [28/100], Step [900/50000], Loss: 1.7823\n",
      "Epoch [28/100], Step [1000/50000], Loss: 1.9682\n",
      "Epoch [29/100], Step [100/50000], Loss: 1.8614\n",
      "Epoch [29/100], Step [200/50000], Loss: 1.9461\n",
      "Epoch [29/100], Step [300/50000], Loss: 1.9271\n",
      "Epoch [29/100], Step [400/50000], Loss: 1.9444\n",
      "Epoch [29/100], Step [500/50000], Loss: 1.7634\n",
      "Epoch [29/100], Step [600/50000], Loss: 1.7952\n",
      "Epoch [29/100], Step [700/50000], Loss: 1.8836\n",
      "Epoch [29/100], Step [800/50000], Loss: 1.9479\n",
      "Epoch [29/100], Step [900/50000], Loss: 1.9470\n",
      "Epoch [29/100], Step [1000/50000], Loss: 1.8264\n",
      "Epoch [30/100], Step [100/50000], Loss: 1.8246\n",
      "Epoch [30/100], Step [200/50000], Loss: 1.8347\n",
      "Epoch [30/100], Step [300/50000], Loss: 1.7661\n",
      "Epoch [30/100], Step [400/50000], Loss: 1.8809\n",
      "Epoch [30/100], Step [500/50000], Loss: 1.8451\n",
      "Epoch [30/100], Step [600/50000], Loss: 1.8245\n",
      "Epoch [30/100], Step [700/50000], Loss: 1.9330\n",
      "Epoch [30/100], Step [800/50000], Loss: 1.9325\n",
      "Epoch [30/100], Step [900/50000], Loss: 1.7730\n",
      "Epoch [30/100], Step [1000/50000], Loss: 1.9011\n",
      "Epoch [31/100], Step [100/50000], Loss: 1.9103\n",
      "Epoch [31/100], Step [200/50000], Loss: 1.8523\n",
      "Epoch [31/100], Step [300/50000], Loss: 1.7745\n",
      "Epoch [31/100], Step [400/50000], Loss: 1.9627\n",
      "Epoch [31/100], Step [500/50000], Loss: 1.9673\n",
      "Epoch [31/100], Step [600/50000], Loss: 1.8074\n",
      "Epoch [31/100], Step [700/50000], Loss: 1.9111\n",
      "Epoch [31/100], Step [800/50000], Loss: 1.8269\n",
      "Epoch [31/100], Step [900/50000], Loss: 1.9016\n",
      "Epoch [31/100], Step [1000/50000], Loss: 1.8481\n",
      "Epoch [32/100], Step [100/50000], Loss: 1.8179\n",
      "Epoch [32/100], Step [200/50000], Loss: 1.7647\n",
      "Epoch [32/100], Step [300/50000], Loss: 1.8419\n",
      "Epoch [32/100], Step [400/50000], Loss: 1.7718\n",
      "Epoch [32/100], Step [500/50000], Loss: 1.7246\n",
      "Epoch [32/100], Step [600/50000], Loss: 1.8223\n",
      "Epoch [32/100], Step [700/50000], Loss: 1.9220\n",
      "Epoch [32/100], Step [800/50000], Loss: 1.8723\n",
      "Epoch [32/100], Step [900/50000], Loss: 1.8263\n",
      "Epoch [32/100], Step [1000/50000], Loss: 1.6996\n",
      "Epoch [33/100], Step [100/50000], Loss: 1.8914\n",
      "Epoch [33/100], Step [200/50000], Loss: 1.8221\n",
      "Epoch [33/100], Step [300/50000], Loss: 1.7381\n",
      "Epoch [33/100], Step [400/50000], Loss: 1.9624\n",
      "Epoch [33/100], Step [500/50000], Loss: 1.7612\n",
      "Epoch [33/100], Step [600/50000], Loss: 1.8213\n",
      "Epoch [33/100], Step [700/50000], Loss: 1.8126\n",
      "Epoch [33/100], Step [800/50000], Loss: 1.7781\n",
      "Epoch [33/100], Step [900/50000], Loss: 1.8862\n",
      "Epoch [33/100], Step [1000/50000], Loss: 1.8804\n",
      "Epoch [34/100], Step [100/50000], Loss: 1.7797\n",
      "Epoch [34/100], Step [200/50000], Loss: 1.8341\n",
      "Epoch [34/100], Step [300/50000], Loss: 1.7941\n",
      "Epoch [34/100], Step [400/50000], Loss: 1.8602\n",
      "Epoch [34/100], Step [500/50000], Loss: 1.7588\n",
      "Epoch [34/100], Step [600/50000], Loss: 1.9046\n",
      "Epoch [34/100], Step [700/50000], Loss: 1.8656\n",
      "Epoch [34/100], Step [800/50000], Loss: 1.7951\n",
      "Epoch [34/100], Step [900/50000], Loss: 1.8100\n",
      "Epoch [34/100], Step [1000/50000], Loss: 1.9073\n",
      "Epoch [35/100], Step [100/50000], Loss: 1.8919\n",
      "Epoch [35/100], Step [200/50000], Loss: 1.7034\n",
      "Epoch [35/100], Step [300/50000], Loss: 1.8024\n",
      "Epoch [35/100], Step [400/50000], Loss: 1.8460\n",
      "Epoch [35/100], Step [500/50000], Loss: 1.7421\n",
      "Epoch [35/100], Step [600/50000], Loss: 1.7662\n",
      "Epoch [35/100], Step [700/50000], Loss: 1.8726\n",
      "Epoch [35/100], Step [800/50000], Loss: 1.8929\n",
      "Epoch [35/100], Step [900/50000], Loss: 1.9023\n",
      "Epoch [35/100], Step [1000/50000], Loss: 1.9203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Step [100/50000], Loss: 1.8177\n",
      "Epoch [36/100], Step [200/50000], Loss: 1.7343\n",
      "Epoch [36/100], Step [300/50000], Loss: 1.8086\n",
      "Epoch [36/100], Step [400/50000], Loss: 1.7932\n",
      "Epoch [36/100], Step [500/50000], Loss: 1.8896\n",
      "Epoch [36/100], Step [600/50000], Loss: 1.8268\n",
      "Epoch [36/100], Step [700/50000], Loss: 1.9105\n",
      "Epoch [36/100], Step [800/50000], Loss: 1.8915\n",
      "Epoch [36/100], Step [900/50000], Loss: 1.8371\n",
      "Epoch [36/100], Step [1000/50000], Loss: 1.8322\n",
      "Epoch [37/100], Step [100/50000], Loss: 1.7863\n",
      "Epoch [37/100], Step [200/50000], Loss: 1.8433\n",
      "Epoch [37/100], Step [300/50000], Loss: 1.7767\n",
      "Epoch [37/100], Step [400/50000], Loss: 1.8531\n",
      "Epoch [37/100], Step [500/50000], Loss: 1.7911\n",
      "Epoch [37/100], Step [600/50000], Loss: 1.7620\n",
      "Epoch [37/100], Step [700/50000], Loss: 1.8591\n",
      "Epoch [37/100], Step [800/50000], Loss: 1.7217\n",
      "Epoch [37/100], Step [900/50000], Loss: 1.8749\n",
      "Epoch [37/100], Step [1000/50000], Loss: 1.7109\n",
      "Epoch [38/100], Step [100/50000], Loss: 1.8043\n",
      "Epoch [38/100], Step [200/50000], Loss: 1.7985\n",
      "Epoch [38/100], Step [300/50000], Loss: 1.7551\n",
      "Epoch [38/100], Step [400/50000], Loss: 1.8594\n",
      "Epoch [38/100], Step [500/50000], Loss: 1.7938\n",
      "Epoch [38/100], Step [600/50000], Loss: 1.8972\n",
      "Epoch [38/100], Step [700/50000], Loss: 1.8422\n",
      "Epoch [38/100], Step [800/50000], Loss: 1.8741\n",
      "Epoch [38/100], Step [900/50000], Loss: 1.7665\n",
      "Epoch [38/100], Step [1000/50000], Loss: 1.8004\n",
      "Epoch [39/100], Step [100/50000], Loss: 1.7775\n",
      "Epoch [39/100], Step [200/50000], Loss: 1.8070\n",
      "Epoch [39/100], Step [300/50000], Loss: 1.7095\n",
      "Epoch [39/100], Step [400/50000], Loss: 1.7609\n",
      "Epoch [39/100], Step [500/50000], Loss: 1.7209\n",
      "Epoch [39/100], Step [600/50000], Loss: 1.8023\n",
      "Epoch [39/100], Step [700/50000], Loss: 1.8187\n",
      "Epoch [39/100], Step [800/50000], Loss: 1.7487\n",
      "Epoch [39/100], Step [900/50000], Loss: 1.7940\n",
      "Epoch [39/100], Step [1000/50000], Loss: 1.6474\n",
      "Epoch [40/100], Step [100/50000], Loss: 1.7149\n",
      "Epoch [40/100], Step [200/50000], Loss: 1.7537\n",
      "Epoch [40/100], Step [300/50000], Loss: 1.7619\n",
      "Epoch [40/100], Step [400/50000], Loss: 1.7241\n",
      "Epoch [40/100], Step [500/50000], Loss: 1.8173\n",
      "Epoch [40/100], Step [600/50000], Loss: 1.8155\n",
      "Epoch [40/100], Step [700/50000], Loss: 1.7634\n",
      "Epoch [40/100], Step [800/50000], Loss: 1.7173\n",
      "Epoch [40/100], Step [900/50000], Loss: 1.8288\n",
      "Epoch [40/100], Step [1000/50000], Loss: 1.8861\n",
      "Epoch [41/100], Step [100/50000], Loss: 1.7802\n",
      "Epoch [41/100], Step [200/50000], Loss: 1.8277\n",
      "Epoch [41/100], Step [300/50000], Loss: 1.7224\n",
      "Epoch [41/100], Step [400/50000], Loss: 1.8185\n",
      "Epoch [41/100], Step [500/50000], Loss: 1.7513\n",
      "Epoch [41/100], Step [600/50000], Loss: 1.8484\n",
      "Epoch [41/100], Step [700/50000], Loss: 1.7372\n",
      "Epoch [41/100], Step [800/50000], Loss: 1.8001\n",
      "Epoch [41/100], Step [900/50000], Loss: 1.7697\n",
      "Epoch [41/100], Step [1000/50000], Loss: 1.7330\n",
      "Epoch [42/100], Step [100/50000], Loss: 1.8180\n",
      "Epoch [42/100], Step [200/50000], Loss: 1.7486\n",
      "Epoch [42/100], Step [300/50000], Loss: 1.8465\n",
      "Epoch [42/100], Step [400/50000], Loss: 1.8248\n",
      "Epoch [42/100], Step [500/50000], Loss: 1.7714\n",
      "Epoch [42/100], Step [600/50000], Loss: 1.7610\n",
      "Epoch [42/100], Step [700/50000], Loss: 1.6181\n",
      "Epoch [42/100], Step [800/50000], Loss: 1.7621\n",
      "Epoch [42/100], Step [900/50000], Loss: 1.7596\n",
      "Epoch [42/100], Step [1000/50000], Loss: 1.7743\n",
      "Epoch [43/100], Step [100/50000], Loss: 1.7084\n",
      "Epoch [43/100], Step [200/50000], Loss: 1.7594\n",
      "Epoch [43/100], Step [300/50000], Loss: 1.6985\n",
      "Epoch [43/100], Step [400/50000], Loss: 1.7919\n",
      "Epoch [43/100], Step [500/50000], Loss: 1.7860\n",
      "Epoch [43/100], Step [600/50000], Loss: 1.7904\n",
      "Epoch [43/100], Step [700/50000], Loss: 1.7129\n",
      "Epoch [43/100], Step [800/50000], Loss: 1.8007\n",
      "Epoch [43/100], Step [900/50000], Loss: 1.7337\n",
      "Epoch [43/100], Step [1000/50000], Loss: 1.8927\n",
      "Epoch [44/100], Step [100/50000], Loss: 1.7446\n",
      "Epoch [44/100], Step [200/50000], Loss: 1.6760\n",
      "Epoch [44/100], Step [300/50000], Loss: 1.7944\n",
      "Epoch [44/100], Step [400/50000], Loss: 1.6638\n",
      "Epoch [44/100], Step [500/50000], Loss: 1.6604\n",
      "Epoch [44/100], Step [600/50000], Loss: 1.8996\n",
      "Epoch [44/100], Step [700/50000], Loss: 1.7892\n",
      "Epoch [44/100], Step [800/50000], Loss: 1.6535\n",
      "Epoch [44/100], Step [900/50000], Loss: 1.7857\n",
      "Epoch [44/100], Step [1000/50000], Loss: 1.6950\n",
      "Epoch [45/100], Step [100/50000], Loss: 1.7220\n",
      "Epoch [45/100], Step [200/50000], Loss: 1.8394\n",
      "Epoch [45/100], Step [300/50000], Loss: 1.6984\n",
      "Epoch [45/100], Step [400/50000], Loss: 1.7165\n",
      "Epoch [45/100], Step [500/50000], Loss: 1.6635\n",
      "Epoch [45/100], Step [600/50000], Loss: 1.6268\n",
      "Epoch [45/100], Step [700/50000], Loss: 1.6068\n",
      "Epoch [45/100], Step [800/50000], Loss: 1.8514\n",
      "Epoch [45/100], Step [900/50000], Loss: 1.8620\n",
      "Epoch [45/100], Step [1000/50000], Loss: 1.7201\n",
      "Epoch [46/100], Step [100/50000], Loss: 1.6615\n",
      "Epoch [46/100], Step [200/50000], Loss: 1.7462\n",
      "Epoch [46/100], Step [300/50000], Loss: 1.7779\n",
      "Epoch [46/100], Step [400/50000], Loss: 1.8907\n",
      "Epoch [46/100], Step [500/50000], Loss: 1.7587\n",
      "Epoch [46/100], Step [600/50000], Loss: 1.8018\n",
      "Epoch [46/100], Step [700/50000], Loss: 1.7319\n",
      "Epoch [46/100], Step [800/50000], Loss: 1.7383\n",
      "Epoch [46/100], Step [900/50000], Loss: 1.7448\n",
      "Epoch [46/100], Step [1000/50000], Loss: 1.6423\n",
      "Epoch [47/100], Step [100/50000], Loss: 1.7994\n",
      "Epoch [47/100], Step [200/50000], Loss: 1.7130\n",
      "Epoch [47/100], Step [300/50000], Loss: 1.7779\n",
      "Epoch [47/100], Step [400/50000], Loss: 1.7853\n",
      "Epoch [47/100], Step [500/50000], Loss: 1.7431\n",
      "Epoch [47/100], Step [600/50000], Loss: 1.6818\n",
      "Epoch [47/100], Step [700/50000], Loss: 1.7761\n",
      "Epoch [47/100], Step [800/50000], Loss: 1.6898\n",
      "Epoch [47/100], Step [900/50000], Loss: 1.8355\n",
      "Epoch [47/100], Step [1000/50000], Loss: 1.8124\n",
      "Epoch [48/100], Step [100/50000], Loss: 1.7890\n",
      "Epoch [48/100], Step [200/50000], Loss: 1.6730\n",
      "Epoch [48/100], Step [300/50000], Loss: 1.6988\n",
      "Epoch [48/100], Step [400/50000], Loss: 1.7967\n",
      "Epoch [48/100], Step [500/50000], Loss: 1.8203\n",
      "Epoch [48/100], Step [600/50000], Loss: 1.7037\n",
      "Epoch [48/100], Step [700/50000], Loss: 1.7313\n",
      "Epoch [48/100], Step [800/50000], Loss: 1.7629\n",
      "Epoch [48/100], Step [900/50000], Loss: 1.7950\n",
      "Epoch [48/100], Step [1000/50000], Loss: 1.6843\n",
      "Epoch [49/100], Step [100/50000], Loss: 1.6950\n",
      "Epoch [49/100], Step [200/50000], Loss: 1.7399\n",
      "Epoch [49/100], Step [300/50000], Loss: 1.5989\n",
      "Epoch [49/100], Step [400/50000], Loss: 1.6526\n",
      "Epoch [49/100], Step [500/50000], Loss: 1.6893\n",
      "Epoch [49/100], Step [600/50000], Loss: 1.7751\n",
      "Epoch [49/100], Step [700/50000], Loss: 1.7689\n",
      "Epoch [49/100], Step [800/50000], Loss: 1.6554\n",
      "Epoch [49/100], Step [900/50000], Loss: 1.6814\n",
      "Epoch [49/100], Step [1000/50000], Loss: 1.8794\n",
      "Epoch [50/100], Step [100/50000], Loss: 1.7318\n",
      "Epoch [50/100], Step [200/50000], Loss: 1.6478\n",
      "Epoch [50/100], Step [300/50000], Loss: 1.6686\n",
      "Epoch [50/100], Step [400/50000], Loss: 1.6501\n",
      "Epoch [50/100], Step [500/50000], Loss: 1.8078\n",
      "Epoch [50/100], Step [600/50000], Loss: 1.8157\n",
      "Epoch [50/100], Step [700/50000], Loss: 1.6370\n",
      "Epoch [50/100], Step [800/50000], Loss: 1.7339\n",
      "Epoch [50/100], Step [900/50000], Loss: 1.6985\n",
      "Epoch [50/100], Step [1000/50000], Loss: 1.7000\n",
      "Epoch [51/100], Step [100/50000], Loss: 1.6948\n",
      "Epoch [51/100], Step [200/50000], Loss: 1.7830\n",
      "Epoch [51/100], Step [300/50000], Loss: 1.7359\n",
      "Epoch [51/100], Step [400/50000], Loss: 1.6423\n",
      "Epoch [51/100], Step [500/50000], Loss: 1.7755\n",
      "Epoch [51/100], Step [600/50000], Loss: 1.6427\n",
      "Epoch [51/100], Step [700/50000], Loss: 1.6870\n",
      "Epoch [51/100], Step [800/50000], Loss: 1.6792\n",
      "Epoch [51/100], Step [900/50000], Loss: 1.6572\n",
      "Epoch [51/100], Step [1000/50000], Loss: 1.7513\n",
      "Epoch [52/100], Step [100/50000], Loss: 1.6047\n",
      "Epoch [52/100], Step [200/50000], Loss: 1.6261\n",
      "Epoch [52/100], Step [300/50000], Loss: 1.7325\n",
      "Epoch [52/100], Step [400/50000], Loss: 1.7129\n",
      "Epoch [52/100], Step [500/50000], Loss: 1.7010\n",
      "Epoch [52/100], Step [600/50000], Loss: 1.6274\n",
      "Epoch [52/100], Step [700/50000], Loss: 1.6973\n",
      "Epoch [52/100], Step [800/50000], Loss: 1.7444\n",
      "Epoch [52/100], Step [900/50000], Loss: 1.7376\n",
      "Epoch [52/100], Step [1000/50000], Loss: 1.6375\n",
      "Epoch [53/100], Step [100/50000], Loss: 1.7830\n",
      "Epoch [53/100], Step [200/50000], Loss: 1.7326\n",
      "Epoch [53/100], Step [300/50000], Loss: 1.7448\n",
      "Epoch [53/100], Step [400/50000], Loss: 1.7165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [500/50000], Loss: 1.6041\n",
      "Epoch [53/100], Step [600/50000], Loss: 1.7013\n",
      "Epoch [53/100], Step [700/50000], Loss: 1.7934\n",
      "Epoch [53/100], Step [800/50000], Loss: 1.7185\n",
      "Epoch [53/100], Step [900/50000], Loss: 1.7110\n",
      "Epoch [53/100], Step [1000/50000], Loss: 1.6807\n",
      "Epoch [54/100], Step [100/50000], Loss: 1.6566\n",
      "Epoch [54/100], Step [200/50000], Loss: 1.7806\n",
      "Epoch [54/100], Step [300/50000], Loss: 1.7819\n",
      "Epoch [54/100], Step [400/50000], Loss: 1.6588\n",
      "Epoch [54/100], Step [500/50000], Loss: 1.6568\n",
      "Epoch [54/100], Step [600/50000], Loss: 1.7690\n",
      "Epoch [54/100], Step [700/50000], Loss: 1.7671\n",
      "Epoch [54/100], Step [800/50000], Loss: 1.7345\n",
      "Epoch [54/100], Step [900/50000], Loss: 1.8610\n",
      "Epoch [54/100], Step [1000/50000], Loss: 1.6685\n",
      "Epoch [55/100], Step [100/50000], Loss: 1.6131\n",
      "Epoch [55/100], Step [200/50000], Loss: 1.7003\n",
      "Epoch [55/100], Step [300/50000], Loss: 1.6576\n",
      "Epoch [55/100], Step [400/50000], Loss: 1.6847\n",
      "Epoch [55/100], Step [500/50000], Loss: 1.6924\n",
      "Epoch [55/100], Step [600/50000], Loss: 1.7536\n",
      "Epoch [55/100], Step [700/50000], Loss: 1.7838\n",
      "Epoch [55/100], Step [800/50000], Loss: 1.6787\n",
      "Epoch [55/100], Step [900/50000], Loss: 1.7607\n",
      "Epoch [55/100], Step [1000/50000], Loss: 1.6887\n",
      "Epoch [56/100], Step [100/50000], Loss: 1.5800\n",
      "Epoch [56/100], Step [200/50000], Loss: 1.7056\n",
      "Epoch [56/100], Step [300/50000], Loss: 1.7446\n",
      "Epoch [56/100], Step [400/50000], Loss: 1.6988\n",
      "Epoch [56/100], Step [500/50000], Loss: 1.7402\n",
      "Epoch [56/100], Step [600/50000], Loss: 1.7256\n",
      "Epoch [56/100], Step [700/50000], Loss: 1.5934\n",
      "Epoch [56/100], Step [800/50000], Loss: 1.7505\n",
      "Epoch [56/100], Step [900/50000], Loss: 1.7509\n",
      "Epoch [56/100], Step [1000/50000], Loss: 1.7375\n",
      "Epoch [57/100], Step [100/50000], Loss: 1.7052\n",
      "Epoch [57/100], Step [200/50000], Loss: 1.7193\n",
      "Epoch [57/100], Step [300/50000], Loss: 1.7618\n",
      "Epoch [57/100], Step [400/50000], Loss: 1.5820\n",
      "Epoch [57/100], Step [500/50000], Loss: 1.6876\n",
      "Epoch [57/100], Step [600/50000], Loss: 1.6521\n",
      "Epoch [57/100], Step [700/50000], Loss: 1.5894\n",
      "Epoch [57/100], Step [800/50000], Loss: 1.7347\n",
      "Epoch [57/100], Step [900/50000], Loss: 1.6020\n",
      "Epoch [57/100], Step [1000/50000], Loss: 1.7182\n",
      "Epoch [58/100], Step [100/50000], Loss: 1.7263\n",
      "Epoch [58/100], Step [200/50000], Loss: 1.5557\n",
      "Epoch [58/100], Step [300/50000], Loss: 1.6544\n",
      "Epoch [58/100], Step [400/50000], Loss: 1.6777\n",
      "Epoch [58/100], Step [500/50000], Loss: 1.6862\n",
      "Epoch [58/100], Step [600/50000], Loss: 1.6855\n",
      "Epoch [58/100], Step [700/50000], Loss: 1.6316\n",
      "Epoch [58/100], Step [800/50000], Loss: 1.6994\n",
      "Epoch [58/100], Step [900/50000], Loss: 1.6562\n",
      "Epoch [58/100], Step [1000/50000], Loss: 1.6609\n",
      "Epoch [59/100], Step [100/50000], Loss: 1.6685\n",
      "Epoch [59/100], Step [200/50000], Loss: 1.6336\n",
      "Epoch [59/100], Step [300/50000], Loss: 1.6546\n",
      "Epoch [59/100], Step [400/50000], Loss: 1.7672\n",
      "Epoch [59/100], Step [500/50000], Loss: 1.6466\n",
      "Epoch [59/100], Step [600/50000], Loss: 1.6196\n",
      "Epoch [59/100], Step [700/50000], Loss: 1.7732\n",
      "Epoch [59/100], Step [800/50000], Loss: 1.6802\n",
      "Epoch [59/100], Step [900/50000], Loss: 1.6912\n",
      "Epoch [59/100], Step [1000/50000], Loss: 1.6370\n",
      "Epoch [60/100], Step [100/50000], Loss: 1.7404\n",
      "Epoch [60/100], Step [200/50000], Loss: 1.7938\n",
      "Epoch [60/100], Step [300/50000], Loss: 1.6780\n",
      "Epoch [60/100], Step [400/50000], Loss: 1.7922\n",
      "Epoch [60/100], Step [500/50000], Loss: 1.6721\n",
      "Epoch [60/100], Step [600/50000], Loss: 1.5630\n",
      "Epoch [60/100], Step [700/50000], Loss: 1.7283\n",
      "Epoch [60/100], Step [800/50000], Loss: 1.7078\n",
      "Epoch [60/100], Step [900/50000], Loss: 1.7569\n",
      "Epoch [60/100], Step [1000/50000], Loss: 1.5921\n",
      "Epoch [61/100], Step [100/50000], Loss: 1.6212\n",
      "Epoch [61/100], Step [200/50000], Loss: 1.7645\n",
      "Epoch [61/100], Step [300/50000], Loss: 1.7177\n",
      "Epoch [61/100], Step [400/50000], Loss: 1.6763\n",
      "Epoch [61/100], Step [500/50000], Loss: 1.6633\n",
      "Epoch [61/100], Step [600/50000], Loss: 1.6996\n",
      "Epoch [61/100], Step [700/50000], Loss: 1.6781\n",
      "Epoch [61/100], Step [800/50000], Loss: 1.6939\n",
      "Epoch [61/100], Step [900/50000], Loss: 1.7336\n",
      "Epoch [61/100], Step [1000/50000], Loss: 1.8373\n",
      "Epoch [62/100], Step [100/50000], Loss: 1.7899\n",
      "Epoch [62/100], Step [200/50000], Loss: 1.6999\n",
      "Epoch [62/100], Step [300/50000], Loss: 1.7329\n",
      "Epoch [62/100], Step [400/50000], Loss: 1.7124\n",
      "Epoch [62/100], Step [500/50000], Loss: 1.7478\n",
      "Epoch [62/100], Step [600/50000], Loss: 1.7614\n",
      "Epoch [62/100], Step [700/50000], Loss: 1.6827\n",
      "Epoch [62/100], Step [800/50000], Loss: 1.6370\n",
      "Epoch [62/100], Step [900/50000], Loss: 1.8022\n",
      "Epoch [62/100], Step [1000/50000], Loss: 1.7178\n",
      "Epoch [63/100], Step [100/50000], Loss: 1.7471\n",
      "Epoch [63/100], Step [200/50000], Loss: 1.6764\n",
      "Epoch [63/100], Step [300/50000], Loss: 1.5626\n",
      "Epoch [63/100], Step [400/50000], Loss: 1.7049\n",
      "Epoch [63/100], Step [500/50000], Loss: 1.7598\n",
      "Epoch [63/100], Step [600/50000], Loss: 1.6055\n",
      "Epoch [63/100], Step [700/50000], Loss: 1.6866\n",
      "Epoch [63/100], Step [800/50000], Loss: 1.5828\n",
      "Epoch [63/100], Step [900/50000], Loss: 1.8632\n",
      "Epoch [63/100], Step [1000/50000], Loss: 1.7077\n",
      "Epoch [64/100], Step [100/50000], Loss: 1.5245\n",
      "Epoch [64/100], Step [200/50000], Loss: 1.7015\n",
      "Epoch [64/100], Step [300/50000], Loss: 1.5879\n",
      "Epoch [64/100], Step [400/50000], Loss: 1.7086\n",
      "Epoch [64/100], Step [500/50000], Loss: 1.5778\n",
      "Epoch [64/100], Step [600/50000], Loss: 1.6241\n",
      "Epoch [64/100], Step [700/50000], Loss: 1.7404\n",
      "Epoch [64/100], Step [800/50000], Loss: 1.6927\n",
      "Epoch [64/100], Step [900/50000], Loss: 1.5789\n",
      "Epoch [64/100], Step [1000/50000], Loss: 1.6518\n",
      "Epoch [65/100], Step [100/50000], Loss: 1.6570\n",
      "Epoch [65/100], Step [200/50000], Loss: 1.7164\n",
      "Epoch [65/100], Step [300/50000], Loss: 1.7392\n",
      "Epoch [65/100], Step [400/50000], Loss: 1.6643\n",
      "Epoch [65/100], Step [500/50000], Loss: 1.7340\n",
      "Epoch [65/100], Step [600/50000], Loss: 1.6366\n",
      "Epoch [65/100], Step [700/50000], Loss: 1.7797\n",
      "Epoch [65/100], Step [800/50000], Loss: 1.6526\n",
      "Epoch [65/100], Step [900/50000], Loss: 1.7018\n",
      "Epoch [65/100], Step [1000/50000], Loss: 1.6664\n",
      "Epoch [66/100], Step [100/50000], Loss: 1.6443\n",
      "Epoch [66/100], Step [200/50000], Loss: 1.6484\n",
      "Epoch [66/100], Step [300/50000], Loss: 1.6224\n",
      "Epoch [66/100], Step [400/50000], Loss: 1.6234\n",
      "Epoch [66/100], Step [500/50000], Loss: 1.7039\n",
      "Epoch [66/100], Step [600/50000], Loss: 1.7049\n",
      "Epoch [66/100], Step [700/50000], Loss: 1.6597\n",
      "Epoch [66/100], Step [800/50000], Loss: 1.6397\n",
      "Epoch [66/100], Step [900/50000], Loss: 1.6785\n",
      "Epoch [66/100], Step [1000/50000], Loss: 1.6422\n",
      "Epoch [67/100], Step [100/50000], Loss: 1.7403\n",
      "Epoch [67/100], Step [200/50000], Loss: 1.7351\n",
      "Epoch [67/100], Step [300/50000], Loss: 1.6557\n",
      "Epoch [67/100], Step [400/50000], Loss: 1.7221\n",
      "Epoch [67/100], Step [500/50000], Loss: 1.5946\n",
      "Epoch [67/100], Step [600/50000], Loss: 1.7134\n",
      "Epoch [67/100], Step [700/50000], Loss: 1.7078\n",
      "Epoch [67/100], Step [800/50000], Loss: 1.6597\n",
      "Epoch [67/100], Step [900/50000], Loss: 1.6718\n",
      "Epoch [67/100], Step [1000/50000], Loss: 1.7111\n",
      "Epoch [68/100], Step [100/50000], Loss: 1.6434\n",
      "Epoch [68/100], Step [200/50000], Loss: 1.6830\n",
      "Epoch [68/100], Step [300/50000], Loss: 1.6584\n",
      "Epoch [68/100], Step [400/50000], Loss: 1.6394\n",
      "Epoch [68/100], Step [500/50000], Loss: 1.6448\n",
      "Epoch [68/100], Step [600/50000], Loss: 1.7065\n",
      "Epoch [68/100], Step [700/50000], Loss: 1.6467\n",
      "Epoch [68/100], Step [800/50000], Loss: 1.6246\n",
      "Epoch [68/100], Step [900/50000], Loss: 1.6078\n",
      "Epoch [68/100], Step [1000/50000], Loss: 1.7230\n",
      "Epoch [69/100], Step [100/50000], Loss: 1.6479\n",
      "Epoch [69/100], Step [200/50000], Loss: 1.7709\n",
      "Epoch [69/100], Step [300/50000], Loss: 1.7062\n",
      "Epoch [69/100], Step [400/50000], Loss: 1.5618\n",
      "Epoch [69/100], Step [500/50000], Loss: 1.5481\n",
      "Epoch [69/100], Step [600/50000], Loss: 1.6253\n",
      "Epoch [69/100], Step [700/50000], Loss: 1.6477\n",
      "Epoch [69/100], Step [800/50000], Loss: 1.7838\n",
      "Epoch [69/100], Step [900/50000], Loss: 1.8186\n",
      "Epoch [69/100], Step [1000/50000], Loss: 1.6987\n",
      "Epoch [70/100], Step [100/50000], Loss: 1.6801\n",
      "Epoch [70/100], Step [200/50000], Loss: 1.6327\n",
      "Epoch [70/100], Step [300/50000], Loss: 1.6579\n",
      "Epoch [70/100], Step [400/50000], Loss: 1.6587\n",
      "Epoch [70/100], Step [500/50000], Loss: 1.6775\n",
      "Epoch [70/100], Step [600/50000], Loss: 1.6232\n",
      "Epoch [70/100], Step [700/50000], Loss: 1.6988\n",
      "Epoch [70/100], Step [800/50000], Loss: 1.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Step [900/50000], Loss: 1.6202\n",
      "Epoch [70/100], Step [1000/50000], Loss: 1.8091\n",
      "Epoch [71/100], Step [100/50000], Loss: 1.6991\n",
      "Epoch [71/100], Step [200/50000], Loss: 1.7162\n",
      "Epoch [71/100], Step [300/50000], Loss: 1.6275\n",
      "Epoch [71/100], Step [400/50000], Loss: 1.7823\n",
      "Epoch [71/100], Step [500/50000], Loss: 1.5705\n",
      "Epoch [71/100], Step [600/50000], Loss: 1.6676\n",
      "Epoch [71/100], Step [700/50000], Loss: 1.6615\n",
      "Epoch [71/100], Step [800/50000], Loss: 1.6803\n",
      "Epoch [71/100], Step [900/50000], Loss: 1.7998\n",
      "Epoch [71/100], Step [1000/50000], Loss: 1.6261\n",
      "Epoch [72/100], Step [100/50000], Loss: 1.6170\n",
      "Epoch [72/100], Step [200/50000], Loss: 1.6288\n",
      "Epoch [72/100], Step [300/50000], Loss: 1.6590\n",
      "Epoch [72/100], Step [400/50000], Loss: 1.7125\n",
      "Epoch [72/100], Step [500/50000], Loss: 1.6991\n",
      "Epoch [72/100], Step [600/50000], Loss: 1.6450\n",
      "Epoch [72/100], Step [700/50000], Loss: 1.6795\n",
      "Epoch [72/100], Step [800/50000], Loss: 1.6850\n",
      "Epoch [72/100], Step [900/50000], Loss: 1.6177\n",
      "Epoch [72/100], Step [1000/50000], Loss: 1.7028\n",
      "Epoch [73/100], Step [100/50000], Loss: 1.6102\n",
      "Epoch [73/100], Step [200/50000], Loss: 1.6590\n",
      "Epoch [73/100], Step [300/50000], Loss: 1.6711\n",
      "Epoch [73/100], Step [400/50000], Loss: 1.5486\n",
      "Epoch [73/100], Step [500/50000], Loss: 1.6165\n",
      "Epoch [73/100], Step [600/50000], Loss: 1.6663\n",
      "Epoch [73/100], Step [700/50000], Loss: 1.6495\n",
      "Epoch [73/100], Step [800/50000], Loss: 1.6025\n",
      "Epoch [73/100], Step [900/50000], Loss: 1.7070\n",
      "Epoch [73/100], Step [1000/50000], Loss: 1.7603\n",
      "Epoch [74/100], Step [100/50000], Loss: 1.6757\n",
      "Epoch [74/100], Step [200/50000], Loss: 1.6661\n",
      "Epoch [74/100], Step [300/50000], Loss: 1.6939\n",
      "Epoch [74/100], Step [400/50000], Loss: 1.6434\n",
      "Epoch [74/100], Step [500/50000], Loss: 1.6799\n",
      "Epoch [74/100], Step [600/50000], Loss: 1.6070\n",
      "Epoch [74/100], Step [700/50000], Loss: 1.6631\n",
      "Epoch [74/100], Step [800/50000], Loss: 1.5845\n",
      "Epoch [74/100], Step [900/50000], Loss: 1.7192\n",
      "Epoch [74/100], Step [1000/50000], Loss: 1.6638\n",
      "Epoch [75/100], Step [100/50000], Loss: 1.6782\n",
      "Epoch [75/100], Step [200/50000], Loss: 1.5431\n",
      "Epoch [75/100], Step [300/50000], Loss: 1.7513\n",
      "Epoch [75/100], Step [400/50000], Loss: 1.5905\n",
      "Epoch [75/100], Step [500/50000], Loss: 1.6200\n",
      "Epoch [75/100], Step [600/50000], Loss: 1.5830\n",
      "Epoch [75/100], Step [700/50000], Loss: 1.7079\n",
      "Epoch [75/100], Step [800/50000], Loss: 1.6446\n",
      "Epoch [75/100], Step [900/50000], Loss: 1.5825\n",
      "Epoch [75/100], Step [1000/50000], Loss: 1.7393\n",
      "Epoch [76/100], Step [100/50000], Loss: 1.6637\n",
      "Epoch [76/100], Step [200/50000], Loss: 1.6239\n",
      "Epoch [76/100], Step [300/50000], Loss: 1.6213\n",
      "Epoch [76/100], Step [400/50000], Loss: 1.6779\n",
      "Epoch [76/100], Step [500/50000], Loss: 1.6039\n",
      "Epoch [76/100], Step [600/50000], Loss: 1.6629\n",
      "Epoch [76/100], Step [700/50000], Loss: 1.7267\n",
      "Epoch [76/100], Step [800/50000], Loss: 1.7345\n",
      "Epoch [76/100], Step [900/50000], Loss: 1.7518\n",
      "Epoch [76/100], Step [1000/50000], Loss: 1.7146\n",
      "Epoch [77/100], Step [100/50000], Loss: 1.6119\n",
      "Epoch [77/100], Step [200/50000], Loss: 1.6381\n",
      "Epoch [77/100], Step [300/50000], Loss: 1.6225\n",
      "Epoch [77/100], Step [400/50000], Loss: 1.6132\n",
      "Epoch [77/100], Step [500/50000], Loss: 1.6998\n",
      "Epoch [77/100], Step [600/50000], Loss: 1.6425\n",
      "Epoch [77/100], Step [700/50000], Loss: 1.7404\n",
      "Epoch [77/100], Step [800/50000], Loss: 1.5942\n",
      "Epoch [77/100], Step [900/50000], Loss: 1.6831\n",
      "Epoch [77/100], Step [1000/50000], Loss: 1.7117\n",
      "Epoch [78/100], Step [100/50000], Loss: 1.5525\n",
      "Epoch [78/100], Step [200/50000], Loss: 1.6215\n",
      "Epoch [78/100], Step [300/50000], Loss: 1.7032\n",
      "Epoch [78/100], Step [400/50000], Loss: 1.5734\n",
      "Epoch [78/100], Step [500/50000], Loss: 1.6464\n",
      "Epoch [78/100], Step [600/50000], Loss: 1.7485\n",
      "Epoch [78/100], Step [700/50000], Loss: 1.6328\n",
      "Epoch [78/100], Step [800/50000], Loss: 1.6472\n",
      "Epoch [78/100], Step [900/50000], Loss: 1.6820\n",
      "Epoch [78/100], Step [1000/50000], Loss: 1.6014\n",
      "Epoch [79/100], Step [100/50000], Loss: 1.6655\n",
      "Epoch [79/100], Step [200/50000], Loss: 1.6594\n",
      "Epoch [79/100], Step [300/50000], Loss: 1.7203\n",
      "Epoch [79/100], Step [400/50000], Loss: 1.5636\n",
      "Epoch [79/100], Step [500/50000], Loss: 1.6815\n",
      "Epoch [79/100], Step [600/50000], Loss: 1.7273\n",
      "Epoch [79/100], Step [700/50000], Loss: 1.7189\n",
      "Epoch [79/100], Step [800/50000], Loss: 1.6122\n",
      "Epoch [79/100], Step [900/50000], Loss: 1.6352\n",
      "Epoch [79/100], Step [1000/50000], Loss: 1.6147\n",
      "Epoch [80/100], Step [100/50000], Loss: 1.7233\n",
      "Epoch [80/100], Step [200/50000], Loss: 1.8005\n",
      "Epoch [80/100], Step [300/50000], Loss: 1.6011\n",
      "Epoch [80/100], Step [400/50000], Loss: 1.6990\n",
      "Epoch [80/100], Step [500/50000], Loss: 1.6800\n",
      "Epoch [80/100], Step [600/50000], Loss: 1.7458\n",
      "Epoch [80/100], Step [700/50000], Loss: 1.6869\n",
      "Epoch [80/100], Step [800/50000], Loss: 1.5610\n",
      "Epoch [80/100], Step [900/50000], Loss: 1.6000\n",
      "Epoch [80/100], Step [1000/50000], Loss: 1.7001\n",
      "Epoch [81/100], Step [100/50000], Loss: 1.6446\n",
      "Epoch [81/100], Step [200/50000], Loss: 1.6539\n",
      "Epoch [81/100], Step [300/50000], Loss: 1.6648\n",
      "Epoch [81/100], Step [400/50000], Loss: 1.6006\n",
      "Epoch [81/100], Step [500/50000], Loss: 1.7203\n",
      "Epoch [81/100], Step [600/50000], Loss: 1.7214\n",
      "Epoch [81/100], Step [700/50000], Loss: 1.6201\n",
      "Epoch [81/100], Step [800/50000], Loss: 1.6758\n",
      "Epoch [81/100], Step [900/50000], Loss: 1.7278\n",
      "Epoch [81/100], Step [1000/50000], Loss: 1.6795\n",
      "Epoch [82/100], Step [100/50000], Loss: 1.7134\n",
      "Epoch [82/100], Step [200/50000], Loss: 1.6440\n",
      "Epoch [82/100], Step [300/50000], Loss: 1.7019\n",
      "Epoch [82/100], Step [400/50000], Loss: 1.6250\n",
      "Epoch [82/100], Step [500/50000], Loss: 1.6282\n",
      "Epoch [82/100], Step [600/50000], Loss: 1.6492\n",
      "Epoch [82/100], Step [700/50000], Loss: 1.5830\n",
      "Epoch [82/100], Step [800/50000], Loss: 1.7004\n",
      "Epoch [82/100], Step [900/50000], Loss: 1.6342\n",
      "Epoch [82/100], Step [1000/50000], Loss: 1.6636\n",
      "Epoch [83/100], Step [100/50000], Loss: 1.6663\n",
      "Epoch [83/100], Step [200/50000], Loss: 1.6365\n",
      "Epoch [83/100], Step [300/50000], Loss: 1.6377\n",
      "Epoch [83/100], Step [400/50000], Loss: 1.7903\n",
      "Epoch [83/100], Step [500/50000], Loss: 1.6490\n",
      "Epoch [83/100], Step [600/50000], Loss: 1.7065\n",
      "Epoch [83/100], Step [700/50000], Loss: 1.6438\n",
      "Epoch [83/100], Step [800/50000], Loss: 1.7208\n",
      "Epoch [83/100], Step [900/50000], Loss: 1.6814\n",
      "Epoch [83/100], Step [1000/50000], Loss: 1.6603\n",
      "Epoch [84/100], Step [100/50000], Loss: 1.6785\n",
      "Epoch [84/100], Step [200/50000], Loss: 1.6406\n",
      "Epoch [84/100], Step [300/50000], Loss: 1.5718\n",
      "Epoch [84/100], Step [400/50000], Loss: 1.7205\n",
      "Epoch [84/100], Step [500/50000], Loss: 1.6404\n",
      "Epoch [84/100], Step [600/50000], Loss: 1.6481\n",
      "Epoch [84/100], Step [700/50000], Loss: 1.6118\n",
      "Epoch [84/100], Step [800/50000], Loss: 1.6397\n",
      "Epoch [84/100], Step [900/50000], Loss: 1.6225\n",
      "Epoch [84/100], Step [1000/50000], Loss: 1.6997\n",
      "Epoch [85/100], Step [100/50000], Loss: 1.6008\n",
      "Epoch [85/100], Step [200/50000], Loss: 1.5774\n",
      "Epoch [85/100], Step [300/50000], Loss: 1.6223\n",
      "Epoch [85/100], Step [400/50000], Loss: 1.6674\n",
      "Epoch [85/100], Step [500/50000], Loss: 1.7676\n",
      "Epoch [85/100], Step [600/50000], Loss: 1.5914\n",
      "Epoch [85/100], Step [700/50000], Loss: 1.7558\n",
      "Epoch [85/100], Step [800/50000], Loss: 1.6321\n",
      "Epoch [85/100], Step [900/50000], Loss: 1.6477\n",
      "Epoch [85/100], Step [1000/50000], Loss: 1.5457\n",
      "Epoch [86/100], Step [100/50000], Loss: 1.6906\n",
      "Epoch [86/100], Step [200/50000], Loss: 1.7209\n",
      "Epoch [86/100], Step [300/50000], Loss: 1.6023\n",
      "Epoch [86/100], Step [400/50000], Loss: 1.6413\n",
      "Epoch [86/100], Step [500/50000], Loss: 1.6310\n",
      "Epoch [86/100], Step [600/50000], Loss: 1.6831\n",
      "Epoch [86/100], Step [700/50000], Loss: 1.6214\n",
      "Epoch [86/100], Step [800/50000], Loss: 1.8004\n",
      "Epoch [86/100], Step [900/50000], Loss: 1.6399\n",
      "Epoch [86/100], Step [1000/50000], Loss: 1.6626\n",
      "Epoch [87/100], Step [100/50000], Loss: 1.6455\n",
      "Epoch [87/100], Step [200/50000], Loss: 1.7001\n",
      "Epoch [87/100], Step [300/50000], Loss: 1.6807\n",
      "Epoch [87/100], Step [400/50000], Loss: 1.7602\n",
      "Epoch [87/100], Step [500/50000], Loss: 1.6604\n",
      "Epoch [87/100], Step [600/50000], Loss: 1.7561\n",
      "Epoch [87/100], Step [700/50000], Loss: 1.6679\n",
      "Epoch [87/100], Step [800/50000], Loss: 1.6544\n",
      "Epoch [87/100], Step [900/50000], Loss: 1.6212\n",
      "Epoch [87/100], Step [1000/50000], Loss: 1.5735\n",
      "Epoch [88/100], Step [100/50000], Loss: 1.7381\n",
      "Epoch [88/100], Step [200/50000], Loss: 1.6421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Step [300/50000], Loss: 1.6614\n",
      "Epoch [88/100], Step [400/50000], Loss: 1.8124\n",
      "Epoch [88/100], Step [500/50000], Loss: 1.5862\n",
      "Epoch [88/100], Step [600/50000], Loss: 1.6778\n",
      "Epoch [88/100], Step [700/50000], Loss: 1.6409\n",
      "Epoch [88/100], Step [800/50000], Loss: 1.7626\n",
      "Epoch [88/100], Step [900/50000], Loss: 1.6269\n",
      "Epoch [88/100], Step [1000/50000], Loss: 1.6052\n",
      "Epoch [89/100], Step [100/50000], Loss: 1.6908\n",
      "Epoch [89/100], Step [200/50000], Loss: 1.6792\n",
      "Epoch [89/100], Step [300/50000], Loss: 1.7207\n",
      "Epoch [89/100], Step [400/50000], Loss: 1.6921\n",
      "Epoch [89/100], Step [500/50000], Loss: 1.7490\n",
      "Epoch [89/100], Step [600/50000], Loss: 1.5990\n",
      "Epoch [89/100], Step [700/50000], Loss: 1.7422\n",
      "Epoch [89/100], Step [800/50000], Loss: 1.6807\n",
      "Epoch [89/100], Step [900/50000], Loss: 1.6210\n",
      "Epoch [89/100], Step [1000/50000], Loss: 1.8408\n",
      "Epoch [90/100], Step [100/50000], Loss: 1.6390\n",
      "Epoch [90/100], Step [200/50000], Loss: 1.6133\n",
      "Epoch [90/100], Step [300/50000], Loss: 1.5818\n",
      "Epoch [90/100], Step [400/50000], Loss: 1.6986\n",
      "Epoch [90/100], Step [500/50000], Loss: 1.6809\n",
      "Epoch [90/100], Step [600/50000], Loss: 1.6511\n",
      "Epoch [90/100], Step [700/50000], Loss: 1.7027\n",
      "Epoch [90/100], Step [800/50000], Loss: 1.6799\n",
      "Epoch [90/100], Step [900/50000], Loss: 1.7081\n",
      "Epoch [90/100], Step [1000/50000], Loss: 1.6726\n",
      "Epoch [91/100], Step [100/50000], Loss: 1.7259\n",
      "Epoch [91/100], Step [200/50000], Loss: 1.6546\n",
      "Epoch [91/100], Step [300/50000], Loss: 1.7011\n",
      "Epoch [91/100], Step [400/50000], Loss: 1.6569\n",
      "Epoch [91/100], Step [500/50000], Loss: 1.8210\n",
      "Epoch [91/100], Step [600/50000], Loss: 1.7286\n",
      "Epoch [91/100], Step [700/50000], Loss: 1.6102\n",
      "Epoch [91/100], Step [800/50000], Loss: 1.6657\n",
      "Epoch [91/100], Step [900/50000], Loss: 1.7203\n",
      "Epoch [91/100], Step [1000/50000], Loss: 1.6726\n",
      "Epoch [92/100], Step [100/50000], Loss: 1.7180\n",
      "Epoch [92/100], Step [200/50000], Loss: 1.6401\n",
      "Epoch [92/100], Step [300/50000], Loss: 1.6876\n",
      "Epoch [92/100], Step [400/50000], Loss: 1.5234\n",
      "Epoch [92/100], Step [500/50000], Loss: 1.7079\n",
      "Epoch [92/100], Step [600/50000], Loss: 1.7442\n",
      "Epoch [92/100], Step [700/50000], Loss: 1.7206\n",
      "Epoch [92/100], Step [800/50000], Loss: 1.7003\n",
      "Epoch [92/100], Step [900/50000], Loss: 1.6171\n",
      "Epoch [92/100], Step [1000/50000], Loss: 1.6346\n",
      "Epoch [93/100], Step [100/50000], Loss: 1.6879\n",
      "Epoch [93/100], Step [200/50000], Loss: 1.6408\n",
      "Epoch [93/100], Step [300/50000], Loss: 1.6799\n",
      "Epoch [93/100], Step [400/50000], Loss: 1.5920\n",
      "Epoch [93/100], Step [500/50000], Loss: 1.6423\n",
      "Epoch [93/100], Step [600/50000], Loss: 1.6701\n",
      "Epoch [93/100], Step [700/50000], Loss: 1.8432\n",
      "Epoch [93/100], Step [800/50000], Loss: 1.6418\n",
      "Epoch [93/100], Step [900/50000], Loss: 1.8192\n",
      "Epoch [93/100], Step [1000/50000], Loss: 1.7099\n",
      "Epoch [94/100], Step [100/50000], Loss: 1.7062\n",
      "Epoch [94/100], Step [200/50000], Loss: 1.6004\n",
      "Epoch [94/100], Step [300/50000], Loss: 1.6990\n",
      "Epoch [94/100], Step [400/50000], Loss: 1.7193\n",
      "Epoch [94/100], Step [500/50000], Loss: 1.6208\n",
      "Epoch [94/100], Step [600/50000], Loss: 1.6662\n",
      "Epoch [94/100], Step [700/50000], Loss: 1.7069\n",
      "Epoch [94/100], Step [800/50000], Loss: 1.8011\n",
      "Epoch [94/100], Step [900/50000], Loss: 1.7192\n",
      "Epoch [94/100], Step [1000/50000], Loss: 1.6837\n",
      "Epoch [95/100], Step [100/50000], Loss: 1.6966\n",
      "Epoch [95/100], Step [200/50000], Loss: 1.6369\n",
      "Epoch [95/100], Step [300/50000], Loss: 1.5796\n",
      "Epoch [95/100], Step [400/50000], Loss: 1.6588\n",
      "Epoch [95/100], Step [500/50000], Loss: 1.5991\n",
      "Epoch [95/100], Step [600/50000], Loss: 1.6606\n",
      "Epoch [95/100], Step [700/50000], Loss: 1.6135\n",
      "Epoch [95/100], Step [800/50000], Loss: 1.7191\n",
      "Epoch [95/100], Step [900/50000], Loss: 1.6597\n",
      "Epoch [95/100], Step [1000/50000], Loss: 1.7060\n",
      "Epoch [96/100], Step [100/50000], Loss: 1.6405\n",
      "Epoch [96/100], Step [200/50000], Loss: 1.6748\n",
      "Epoch [96/100], Step [300/50000], Loss: 1.6807\n",
      "Epoch [96/100], Step [400/50000], Loss: 1.7097\n",
      "Epoch [96/100], Step [500/50000], Loss: 1.7065\n",
      "Epoch [96/100], Step [600/50000], Loss: 1.6761\n",
      "Epoch [96/100], Step [700/50000], Loss: 1.7211\n",
      "Epoch [96/100], Step [800/50000], Loss: 1.6788\n",
      "Epoch [96/100], Step [900/50000], Loss: 1.6815\n",
      "Epoch [96/100], Step [1000/50000], Loss: 1.8004\n",
      "Epoch [97/100], Step [100/50000], Loss: 1.5812\n",
      "Epoch [97/100], Step [200/50000], Loss: 1.6228\n",
      "Epoch [97/100], Step [300/50000], Loss: 1.6485\n",
      "Epoch [97/100], Step [400/50000], Loss: 1.6382\n",
      "Epoch [97/100], Step [500/50000], Loss: 1.6842\n",
      "Epoch [97/100], Step [600/50000], Loss: 1.5880\n",
      "Epoch [97/100], Step [700/50000], Loss: 1.6232\n",
      "Epoch [97/100], Step [800/50000], Loss: 1.6925\n",
      "Epoch [97/100], Step [900/50000], Loss: 1.6058\n",
      "Epoch [97/100], Step [1000/50000], Loss: 1.6255\n",
      "Epoch [98/100], Step [100/50000], Loss: 1.6488\n",
      "Epoch [98/100], Step [200/50000], Loss: 1.5810\n",
      "Epoch [98/100], Step [300/50000], Loss: 1.6946\n",
      "Epoch [98/100], Step [400/50000], Loss: 1.7092\n",
      "Epoch [98/100], Step [500/50000], Loss: 1.6754\n",
      "Epoch [98/100], Step [600/50000], Loss: 1.6836\n",
      "Epoch [98/100], Step [700/50000], Loss: 1.6023\n",
      "Epoch [98/100], Step [800/50000], Loss: 1.7211\n",
      "Epoch [98/100], Step [900/50000], Loss: 1.7811\n",
      "Epoch [98/100], Step [1000/50000], Loss: 1.5806\n",
      "Epoch [99/100], Step [100/50000], Loss: 1.7424\n",
      "Epoch [99/100], Step [200/50000], Loss: 1.5997\n",
      "Epoch [99/100], Step [300/50000], Loss: 1.8801\n",
      "Epoch [99/100], Step [400/50000], Loss: 1.6702\n",
      "Epoch [99/100], Step [500/50000], Loss: 1.6808\n",
      "Epoch [99/100], Step [600/50000], Loss: 1.6897\n",
      "Epoch [99/100], Step [700/50000], Loss: 1.5812\n",
      "Epoch [99/100], Step [800/50000], Loss: 1.7052\n",
      "Epoch [99/100], Step [900/50000], Loss: 1.6098\n",
      "Epoch [99/100], Step [1000/50000], Loss: 1.7365\n",
      "Epoch [100/100], Step [100/50000], Loss: 1.6220\n",
      "Epoch [100/100], Step [200/50000], Loss: 1.6599\n",
      "Epoch [100/100], Step [300/50000], Loss: 1.6591\n",
      "Epoch [100/100], Step [400/50000], Loss: 1.7007\n",
      "Epoch [100/100], Step [500/50000], Loss: 1.7212\n",
      "Epoch [100/100], Step [600/50000], Loss: 1.6610\n",
      "Epoch [100/100], Step [700/50000], Loss: 1.5796\n",
      "Epoch [100/100], Step [800/50000], Loss: 1.6696\n",
      "Epoch [100/100], Step [900/50000], Loss: 1.6371\n",
      "Epoch [100/100], Step [1000/50000], Loss: 1.7409\n"
     ]
    }
   ],
   "source": [
    "# loss = Variable(torch.cuda.FloatTensor([0]))\n",
    "los =[]\n",
    "epoch=[]\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "#             images = torch.FloatTensor(images)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "#             print(Train_labels_t[i])\n",
    "#             labels = Variable(torch.LongTensor([Train_labels_t[i]])).to(device)\n",
    "        # Forward pass\n",
    "#             print(images.size())\n",
    "#             images = images.unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "#         print(images.size(),outputs.size())\n",
    "#             print(outputs,\"        \",labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        los.append(loss.data[0])\n",
    "            \n",
    "#             print (loss.data[0])\n",
    "#             lo.append(loss_contrastive.data[0])\n",
    "            \n",
    "#             loss_contrastive += Variable(torch.FloatTensor([0]))\n",
    "#             loss_contrastive.backward(retain_graph=True)\n",
    "            \n",
    "#         optimizer.step()\n",
    "#         loss = Variable(torch.cuda.FloatTensor([0]))\n",
    "#         else:\n",
    "#             images = torch.FloatTensor(images)\n",
    "#             images = images.to(device)\n",
    "# #             print(Train_labels_t[i])\n",
    "#             labels = Variable(torch.LongTensor([Train_labels_t[i]])).to(device)\n",
    "#             # Forward pass\n",
    "# #             print(images.size())\n",
    "#             images = images.unsqueeze(0)\n",
    "#             outputs = model(images)\n",
    "# #             print(outputs,\"        \",labels)\n",
    "#             loss += criterion(outputs, labels)\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(los, open(\"los_2.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(los))\n",
    "new_l=[]\n",
    "for i,l in enumerate(los):\n",
    "    if i%1000==0:\n",
    "        new_l.append(l)\n",
    "\n",
    "epoch = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb974aa9f98>]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch,new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
