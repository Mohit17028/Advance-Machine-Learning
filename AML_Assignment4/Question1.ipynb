{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "#         norm = distance_positive.norm(p=2, dim=0, keepdim=True)\n",
    "#         distance_positive = distance_positive.div(norm.expand_as(distance_positive))\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "#         norm2 = distance_negative.norm(p=2, dim=0, keepdim=True)\n",
    "#         distance_negative = distance_negative.div(norm2.expand_as(distance_negative))\n",
    "        losses = F.relu(distance_positive - distance_negative+ self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../AML_Assignment2/cifar/\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as skt\n",
    "\n",
    "def extractImagesAndLabels(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dicte = pickle.load(f,encoding='bytes')\n",
    "    images = dicte[b'data']\n",
    "    Matrix=[]\n",
    "    for i in images:\n",
    "        ingle_img_reshaped =  np.reshape(i,(3, 32,32))\n",
    "        Matrix.append(ingle_img_reshaped)\n",
    "    Matrix = np.array(Matrix)\n",
    "    labels = dicte[b'labels']\n",
    "    labels = np.array(labels)\n",
    "    return Matrix, labels\n",
    "\n",
    "def extractCategories(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dict = pickle.load(f,encoding='bytes')\n",
    "    return dict[b'label_names']\n",
    "\n",
    "def saveCifarImage(array, path, file):\n",
    "    # array is 3x32x32. cv2 needs 32x32x3\n",
    "    array = array.asnumpy().transpose(1,2,0)\n",
    "    # array is RGB. cv2 needs BGR\n",
    "    array = cv2.cvtColor(array, cv2.COLOR_RGB2BGR)\n",
    "    # save to PNG file\n",
    "    return cv2.imwrite(path+file+\".png\", array)\n",
    "\n",
    "\n",
    "Train_data, Train_labels = extractImagesAndLabels(path, \"data_batch_1\")\n",
    "Train_data_2, Train_labels_2 = extractImagesAndLabels(path,\"data_batch_2\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_4, Train_labels_4 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_5, Train_labels_5 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_t = np.concatenate((Train_data, Train_data_2,Train_data_3,Train_data_4,Train_data_5), axis=0)\n",
    "Train_labels_t = np.concatenate((Train_labels,Train_labels_2,Train_labels_3,Train_labels_4,Train_labels_5), axis=0)\n",
    "Test_data, Test_labels = extractImagesAndLabels(path, \"test_batch\")\n",
    "\n",
    "class0,class1,class2,class3,class4,class5,class6,class7,class8,class9 = [],[],[],[],[],[],[],[],[],[]\n",
    "for label,sample in zip(Train_labels,Train_data_t):\n",
    "    if label==0:\n",
    "        class0.append(sample)\n",
    "    elif label ==1: \n",
    "        class1.append(sample)\n",
    "    elif label ==2: \n",
    "        class2.append(sample)\n",
    "    elif label ==3: \n",
    "        class3.append(sample)\n",
    "    elif label ==4: \n",
    "        class4.append(sample)\n",
    "    elif label ==5: \n",
    "        class5.append(sample)\n",
    "    elif label ==6: \n",
    "        class6.append(sample)\n",
    "    elif label ==7: \n",
    "        class7.append(sample)\n",
    "    elif label ==8: \n",
    "        class8.append(sample)\n",
    "    else:\n",
    "        class9.append(sample)\n",
    "\n",
    "\n",
    "# class0,class1,class2,class3,class4,class5,class6,class7,class8,class9 = np.array(class0),np.array(class1),np.array(class2),np.array(class3),np.array(class4),np.array(class5),np.array(class6),np.array(class7),np.array(class8),np.array(class9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005 974 1032 1016 999 937 1030 1001 1025 981\n"
     ]
    }
   ],
   "source": [
    "print(len(class0),len(class1),len(class2),len(class3),len(class4),len(class5),len(class6),len(class7),len(class8),len(class9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_classes = [1005,974,1032,1016,999,937,1030,1001,1025,981]\n",
    "dic= {}\n",
    "dic[0],dic[1],dic[2],dic[3],dic[4],dic[5],dic[6],dic[7],dic[8],dic[9] = class0,class1,class2,class3,class4,class5,class6,class7,class8,class9\n",
    "#Triplet MAKER\n",
    "#5000 samples\n",
    "pair =[]\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def TriplePairMaker(pair):\n",
    "    for i in range(10):\n",
    "        numbers=[k for k in range(len_classes[i])]\n",
    "        shuffle(numbers)\n",
    "        numbers= numbers[:930]\n",
    "        curr_class=-1\n",
    "        for j in range(len(numbers)-1):\n",
    "            if j%93==0:\n",
    "                curr_class+=1\n",
    "                if i!=curr_class:\n",
    "                    temp={}\n",
    "                    temp['I1'],temp['I2'],temp['I3'],temp['C1'],temp['C2'],temp['C3']=numbers[j],numbers[j+1],random.randint(0,200),i,i,curr_class\n",
    "                    pair.append(temp)\n",
    "                    temp2={}\n",
    "                    temp2['I1'],temp2['I2'],temp2['I3'],temp2['C1'],temp2['C2'],temp2['C3']=numbers[j],numbers[j+1],random.randint(200,400),i,i,curr_class\n",
    "                    pair.append(temp2)\n",
    "                    temp3={}\n",
    "                    temp3['I1'],temp3['I2'],temp3['I3'],temp3['C1'],temp3['C2'],temp3['C3']=numbers[j],numbers[j+1],random.randint(400,600),i,i,curr_class\n",
    "                    pair.append(temp3)\n",
    "            elif i==curr_class:\n",
    "                continue\n",
    "            else:\n",
    "                temp={}\n",
    "                temp['I1'],temp['I2'],temp['I3'],temp['C1'],temp['C2'],temp['C3']=numbers[j],numbers[j+1],random.randint(0,200),i,i,curr_class\n",
    "                pair.append(temp)\n",
    "                temp2={}\n",
    "                temp2['I1'],temp2['I2'],temp2['I3'],temp2['C1'],temp2['C2'],temp2['C3']=numbers[j],numbers[j+1],random.randint(200,400),i,i,curr_class\n",
    "                pair.append(temp2)\n",
    "                temp3={}\n",
    "                temp3['I1'],temp3['I2'],temp3['I3'],temp3['C1'],temp3['C2'],temp3['C3']=numbers[j],numbers[j+1],random.randint(400,600),i,i,curr_class\n",
    "                pair.append(temp3)\n",
    "                \n",
    "    return pair\n",
    "\n",
    "pair = TriplePairMaker(pair)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "pair = np.array(pair)\n",
    "train_gallery=[]\n",
    "train_prob =[]\n",
    "test_gallery =[]\n",
    "test_prob=[]\n",
    "print(len(class0)//4)\n",
    "train_gallery= class0[0:len(class0)//4]+class1[0:len(class1)//4]+class2[0:len(class2)//4]+class3[0:len(class3)//4]+class4[0:len(class4)//4]+class5[0:len(class5)//4]+class6[0:len(class6)//4]+class7[0:len(class7)//4]+class8[0:len(class8)//4]+class9[0:len(class9)//4]\n",
    "train_gallery_test =[]\n",
    "for j in range(10):\n",
    "    train_gallery_test += [j for k in range(len_classes[j]//4)]\n",
    "    \n",
    "train_prob= class0[len(class0)//4:len(class0)//2]+class1[len(class1)//4:len(class1)//2]+class2[len(class2)//4:len(class2)//2]+class3[len(class3)//4:len(class3)//2]+class4[len(class4)//4:len(class4)//2]+class5[len(class5)//4:len(class5)//2]+class6[len(class6)//4:len(class6)//2]+class7[len(class7)//4:len(class7)//2]+class8[len(class8)//4:len(class8)//2]+class9[len(class9)//4:len(class9)//2]\n",
    "train_prob_test=[]\n",
    "for j in range(10):\n",
    "    train_prob_test += [j for k in range(len_classes[j]//4,len_classes[j]//2)]\n",
    "\n",
    "    \n",
    "# import torch.utils.data as data_utils\n",
    "\n",
    "# Train_data_t = torch.from_numpy(Train_data_t)\n",
    "# Train_labels_t = torch.from_numpy(Train_labels_t)\n",
    "# Train_labels_t = Train_labels_t.type(torch.FloatTensor)\n",
    "# Train_data_t = Train_data_t.type(torch.FloatTensor)\n",
    "# train_l = data_utils.TensorDataset(Train_data_t, Train_labels_t)\n",
    "# trainLoader= data_utils.DataLoader(train_l, batch_size=16, shuffle=True)\n",
    "# Test_data = torch.from_numpy(Test_data)\n",
    "# Test_labels = torch.from_numpy(Test_labels)\n",
    "# Test_data  = Test_data.type(torch.FloatTensor)\n",
    "# Test_labels = Test_labels.type(torch.FloatTensor)\n",
    "# testLoader = data_utils.TensorDataset(Test_data,Test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_gallery_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.sequence = nn.Sequential(\n",
    "                    nn.Conv2d(3,32,3,1,1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(3,2,1),\n",
    "                    nn.Conv2d(32,32,3,1,1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(3,2,1),\n",
    "                    nn.Conv2d(32,32,3,1,1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(3,2,1),\n",
    "                    nn.Conv2d(32,32,3,1,1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(3,1,1),\n",
    "                    nn.Conv2d(32,32,3,1,1),\n",
    "                    nn.ReLU(),\n",
    "                    Flatten(),\n",
    "                    nn.Linear(512,128),\n",
    "                    )\n",
    "    def forward(self,x):\n",
    "        return self.sequence(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (sequence): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): Flatten()\n",
      "    (15): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "print(model)\n",
    "import torch.optim as optim\n",
    "optimizer= optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testLoader= data_utils.DataLoader(testLoader, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  1 Loss  tensor(0.4807)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  2 Loss  tensor(0.3113)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  3 Loss  tensor(0.3214)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  4 Loss  tensor(0.3721)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  5 Loss  tensor(0.2981)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  6 Loss  tensor(0.2750)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  7 Loss  tensor(0.3133)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  8 Loss  tensor(0.2962)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  9 Loss  tensor(0.2942)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  10 Loss  tensor(0.2854)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  11 Loss  tensor(0.2415)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  12 Loss  tensor(0.1773)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  13 Loss  tensor(0.2396)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  14 Loss  tensor(0.2294)\n",
      "<class 'NoneType'> None\n",
      "None None None\n",
      "Epoch  0 Batch  15 Loss  tensor(0.4136)\n",
      "<class 'NoneType'> None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b671eadf3523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Batch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Loss \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "loss_fn= TripletLoss(0.4)\n",
    "epoch =10\n",
    "acc=[]\n",
    "lo=[]\n",
    "for ep in range(epoch):\n",
    "    batch_anchor=[]\n",
    "    batch_pos=[]\n",
    "    batch_neg=[]\n",
    "    batch=0\n",
    "    for i,p in enumerate(pair):\n",
    "        if i%128==0 and i!=0:\n",
    "            batch+=1\n",
    "            batch_anchor= np.array(batch_anchor)\n",
    "#             print(type(batch_anchor))\n",
    "            batch_anchor= torch.FloatTensor(batch_anchor)\n",
    "#             print(type(batch_anchor))\n",
    "            batch_anchor=Variable(batch_anchor)\n",
    "#             print(type(batch_anchor))\n",
    "#             break\n",
    "#             batch_anchor = F.upsample(batch_anchor,size=(64,64),mode=\"bilinear\")\n",
    "            batch_pos= torch.FloatTensor(np.array(batch_pos))\n",
    "            batch_pos=Variable(batch_pos)\n",
    "#             batch_pos = F.upsample(batch_pos,size=(64,64),mode=\"bilinear\")\n",
    "            batch_neg= torch.FloatTensor(np.array(batch_neg))\n",
    "            batch_neg=Variable(batch_neg)\n",
    "#             batch_neg= F.upsample(batch_neg,size=(64,64),mode=\"bilinear\")\n",
    "#             batch_anchor,batch_pos,batch_neg = Variable(batch_anchor,requires_grad=True),Variable(batch_pos,requires_grad=True),Variable(batch_neg,requires_grad=True)\n",
    "#             print(type(batch_anchor))\n",
    "            out1= model(batch_anchor)\n",
    "            out2 = model(batch_pos)\n",
    "            out3=model(batch_neg)\n",
    "            optimizer.zero_grad()\n",
    "            loss= loss_fn(out1,out2,out3)\n",
    "#             print(loss)\n",
    "            a = loss.retain_grad()\n",
    "            print(type(a), a)\n",
    "            loss.backward()\n",
    "            print(out1.grad,out2.grad,out3.grad)\n",
    "            print(\"Epoch \",ep,\"Batch \",batch,\"Loss \",loss.data)\n",
    "            optimizer.step()\n",
    "            batch_anchor=[]\n",
    "            batch_pos=[]\n",
    "            batch_neg=[]\n",
    "        else:\n",
    "            batch_anchor.append(dic[p['C1']][p['I1']])\n",
    "            batch_pos.append(dic[p['C2']][p['I2']])\n",
    "            batch_neg.append(dic[p['C3']][p['I3']])\n",
    "            \n",
    "    ##tAccuracy for Testing probe:\n",
    "    train_gallery_tensor = Variable(torch.FloatTensor(np.array(train_gallery)))\n",
    "    train_prob_tensor = Variable(torch.FloatTensor(np.array(train_prob)))\n",
    "    out_train = model(train_gallery_tensor)\n",
    "    out_prob=model(train_prob_tensor)\n",
    "    pred=[]\n",
    "    for vec in out_prob:\n",
    "        pred_vec=100000\n",
    "        pred_class=-1\n",
    "        for invec,label in zip(out_train,train_gallery_test):\n",
    "#             print(vec.shape,invec.shape)\n",
    "            distance = (vec - invec)\n",
    "            num = distance.data\n",
    "            num = torch.sum(num**2,dim=0)\n",
    "#             print(num)\n",
    "# #             num= num.pow(2).sum(1)\n",
    "#             print(\"sacsvv   \", type(num))\n",
    "#             break\n",
    "#             print(distance.shape)\n",
    "#             norm = distance.norm(p=2)\n",
    "#             distance = distance.div(norm.expand_as(distance))\n",
    "#             print(\"Normalized  \", distance.data[0])\n",
    "#             distance = distance.div(norm.)\n",
    "            if pred_vec>num:\n",
    "                pred_vec=distance.data[0]\n",
    "                \n",
    "                pred_class=label\n",
    "#                 print(pred_vec,pred_class)\n",
    "#             break\n",
    "        pred.append(pred_class)\n",
    "        \n",
    "#     correct = (torch.LongTensor(np.array(pred)) == torch.LongTensor(np.array(train_prob_test))).sum().item()\n",
    "    c=0\n",
    "    for p,r in zip(pred, train_prob_test):\n",
    "        if p==r:\n",
    "            c+=1\n",
    "    c= c/len(pred)\n",
    "    acc.append(c)\n",
    "    lo.append(loss.data[0])\n",
    "    print(\"Accuracy on Train Set  \",c )\n",
    "                \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9b8c55aa450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "acc=[0.1004,0.138,0.1488,0.1492,0.1224,0.1584,0.1208]\n",
    "loss=[0.4775,0.4309,0.4122,0.4961,0.4005,0.4088,0.3596]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gallery= class0[0:len(class0)//4]+class1[0:len(class1)//4]+class2[0:len(class2)//4]+class3[0:len(class3)//4]+class4[0:len(class4)//4]+class5[0:len(class5)//4]+class6[0:len(class6)//4]+class7[0:len(class7)//4]+class8[0:len(class8)//4]+class9[0:len(class9)//4]\n",
    "test_gallery_test =[]\n",
    "for j in range(10):\n",
    "    train_gallery_test += [j for k in range(len_classes[j]//4)]\n",
    "    \n",
    "test_prob= class0[len(class0)//4:len(class0)//2]+class1[len(class1)//4:len(class1)//2]+class2[len(class2)//4:len(class2)//2]+class3[len(class3)//4:len(class3)//2]+class4[len(class4)//4:len(class4)//2]+class5[len(class5)//4:len(class5)//2]+class6[len(class6)//4:len(class6)//2]+class7[len(class7)//4:len(class7)//2]+class8[len(class8)//4:len(class8)//2]+class9[len(class9)//4:len(class9)//2]\n",
    "test_prob_test=[]\n",
    "for j in range(10):\n",
    "    train_prob_test += [j for k in range(len_classes[j]//4,len_classes[j]//2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_test0,class_test1,class_test2,class_test3,class_test4,class_test5,class_test6,class_test7,class_test8,class_test9 = [],[],[],[],[],[],[],[],[],[]\n",
    "for label,sample in zip(Test_labels,Test_data):\n",
    "    if label==0:\n",
    "        class_test0.append(sample)\n",
    "    elif label ==1: \n",
    "        class_test1.append(sample)\n",
    "    elif label ==2: \n",
    "        class_test2.append(sample)\n",
    "    elif label ==3: \n",
    "        class_test3.append(sample)\n",
    "    elif label ==4: \n",
    "        class_test4.append(sample)\n",
    "    elif label ==5: \n",
    "        class_test5.append(sample)\n",
    "    elif label ==6: \n",
    "        class_test6.append(sample)\n",
    "    elif label ==7: \n",
    "        class_test7.append(sample)\n",
    "    elif label ==8: \n",
    "        class_test8.append(sample)\n",
    "    else:\n",
    "        class_test9.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_ROOT ='../AML_Assignment2/cifar/'\n",
    "# IMG_PATH = DATA_ROOT + 'data_batch_1'\n",
    "# IMG_EXT = '.png'\n",
    "# # IMG_DATA_LABELS = DATA_ROOT + 'batches.meta'\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    j+=2\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
