{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as  F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(nn.Linear(2048,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "            nn.Softmax())\n",
    "# for child in model.children():\n",
    "#     ct+=1\n",
    "#     if ct <= 9:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIFAR Loading ######\n",
    "path = \"../AML_Assignment2/cifar/\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as skt\n",
    "\n",
    "def extractImagesAndLabels(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dicte = pickle.load(f,encoding='bytes')\n",
    "    images = dicte[b'data']\n",
    "    Matrix=[]\n",
    "    for i in images:\n",
    "        ingle_img_reshaped =  np.reshape(i,(3, 32,32))\n",
    "        Matrix.append(ingle_img_reshaped)\n",
    "    Matrix = np.array(Matrix)\n",
    "    labels = dicte[b'labels']\n",
    "    labels = np.array(labels)\n",
    "    return Matrix, labels\n",
    "\n",
    "def extractCategories(path, file):\n",
    "    f = open(path+file, 'rb')\n",
    "    dict = pickle.load(f,encoding='bytes')\n",
    "    return dict[b'label_names']\n",
    "\n",
    "def saveCifarImage(array, path, file):\n",
    "    # array is 3x32x32. cv2 needs 32x32x3\n",
    "    array = array.asnumpy().transpose(1,2,0)\n",
    "    # array is RGB. cv2 needs BGR\n",
    "    array = cv2.cvtColor(array, cv2.COLOR_RGB2BGR)\n",
    "    # save to PNG file\n",
    "    return cv2.imwrite(path+file+\".png\", array)\n",
    "\n",
    "\n",
    "Train_data, Train_labels = extractImagesAndLabels(path, \"data_batch_1\")\n",
    "Train_data_2, Train_labels_2 = extractImagesAndLabels(path,\"data_batch_2\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_3, Train_labels_3 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_4, Train_labels_4 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_5, Train_labels_5 = extractImagesAndLabels(path, \"data_batch_3\")\n",
    "Train_data_t = np.concatenate((Train_data, Train_data_2,Train_data_3,Train_data_4,Train_data_5), axis=0)\n",
    "Train_labels_t = np.concatenate((Train_labels,Train_labels_2,Train_labels_3,Train_labels_4,Train_labels_5), axis=0)\n",
    "Test_data, Test_labels = extractImagesAndLabels(path, \"test_batch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "Train_data_t = torch.from_numpy(Train_data_t)\n",
    "Train_labels_t = torch.from_numpy(Train_labels_t)\n",
    "Train_labels_t = Train_labels_t.type(torch.FloatTensor)\n",
    "Train_data_t = Train_data_t.type(torch.FloatTensor)\n",
    "train_l = data_utils.TensorDataset(Train_data_t, Train_labels_t)\n",
    "trainLoader= data_utils.DataLoader(train_l, batch_size=16, shuffle=True)\n",
    "Test_data = torch.from_numpy(Test_data)\n",
    "Test_labels = torch.from_numpy(Test_labels)\n",
    "Test_data  = Test_data.type(torch.FloatTensor)\n",
    "Test_labels = Test_labels.type(torch.FloatTensor)\n",
    "testLoader = data_utils.TensorDataset(Test_data,Test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoader= data_utils.DataLoader(testLoader, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_smaple = Test_data[0]\n",
    "\n",
    "# Test_smaple = Test_smaple.data.resize_(3,224,224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = nn.CrossEntropyLoss()\n",
    "def train(model_d, device, train_loader, optimizer,epoch):\n",
    "    model_d.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = F.upsample(data, size=(224,224), mode=\"bilinear\")\n",
    "#         print(data.size())\n",
    "#         target = target-1\n",
    "        data,target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_d(data)\n",
    "        loss = criteria(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "def test(model_d, device, test_loader, set_name, contain,acc):\n",
    "    model_d.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = F.upsample(data, size=(224,224), mode=\"bilinear\")\n",
    "            data,target = Variable(data), Variable(target)\n",
    "#             target = target-1\n",
    "            output = model_d(data)\n",
    "            test_loss += criteria(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    contain.append(test_loss)\n",
    "    acc.append(correct/len(test_loader.dataset))\n",
    "    print(\"Accuracy......\")\n",
    "    print('\\n'+set_name+': Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_activate(m):\n",
    "    learning_rate = 0.0001\n",
    "    loss_train=[]\n",
    "    acc_train=[]\n",
    "    loss_test=[]\n",
    "    acc_test=[]\n",
    "    ep=[]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    params = list(m.parameters()) + list(m.fc.parameters())\n",
    "    optimizer = torch.optim.Adam(params,lr=learning_rate)\n",
    "    num_epochs=10\n",
    "    total_step =len(Train_data_t)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#     device='cpu'\n",
    "    print(\"Starting............\")\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        ep.append(epoch)\n",
    "        train(m, device, trainLoader, optimizer, num_epochs)\n",
    "        test(m, device, trainLoader, \"Training_set\",loss_train, acc_train)\n",
    "        test(m, device, testLoader, \"Test_Set\",loss_test,acc_test)\n",
    "    return loss_train,loss_test,acc_test,acc_train,ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(ep, loss_train, loss_test, subject):\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(subject)\n",
    "    plt.plot(ep,loss_train)\n",
    "    plt.plot(ep,loss_test)\n",
    "    plt.legend([\"Loss-Train\", \"Loss-Test\"], loc='upper right')\n",
    "    plt.savefig(\"Figures/\"+subject+\"_loss_plot.png\")\n",
    "\n",
    "def acc_plot(ep,acc_train,acc_test,subject):\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(subject)\n",
    "    plt.plot(ep,acc_train)\n",
    "    plt.plot(ep,acc_test)\n",
    "    plt.legend([\"Acuracy-Train\", \"Accuracy-Test\"], loc='lower right')\n",
    "    plt.savefig(\"Acc_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def gradient_plot(gradient,name):\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    es = [i for i in range(len(gradient))]\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(name)\n",
    "    plt.plot(es,gradient)\n",
    "    # plt.plot(ep,acc_test)\n",
    "    plt.legend([\"Gradient\"], loc='upper right')\n",
    "    plt.savefig(\"Figures/\"+subject+\"_grad_plot.png\")\n",
    "    \n",
    "    \n",
    "def plot_kernels(tensor, name,num_cols=6):\n",
    "    print (tensor.shape)\n",
    "    if not tensor.ndim==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "    if not tensor.shape[-1]==3:\n",
    "        raise Exception(\"last dim needs to be 3 to plot\")\n",
    "    num_kernels = tensor.shape[0]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    print(num_cols,num_rows)\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        p = tensor[i,0]\n",
    "        p = p.flatten()\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(4, 3)\n",
    "#         fig.savefig('test2png.png', dpi=10, forward=True)\n",
    "        plt.hist(p, normed=True, bins=9)\n",
    "        plt.ylabel(\"Weights-Dist\"+name) \n",
    "        plt.savefig(\"Figures/\"+\"hist_layer(\"+name+\").png\")\n",
    "        break\n",
    "    \n",
    "def model_hist_maker(model_name,subject):\n",
    "    filters = model_name.modules\n",
    "    body_model = [i for i in model_name.children()]\n",
    "#     print(body_model)\n",
    "    p = len(body_model)-5\n",
    "    for i in range(p):\n",
    "        layer1 = body_model[i][0]\n",
    "        tensor = layer1.weight.data.cpu().numpy()\n",
    "        plot_kernels(tensor, subject+\"_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/mohit1_aml/anaconda3/envs/mohitpy36/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.961151\n",
      "Train Epoch: 10 [1600/50000 (3%)]\tLoss: 1.961151\n",
      "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 1.886199\n",
      "Train Epoch: 10 [4800/50000 (10%)]\tLoss: 1.789193\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.961151\n",
      "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.773643\n",
      "Train Epoch: 10 [9600/50000 (19%)]\tLoss: 1.711150\n",
      "Train Epoch: 10 [11200/50000 (22%)]\tLoss: 1.961100\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.711147\n",
      "Train Epoch: 10 [14400/50000 (29%)]\tLoss: 1.898369\n",
      "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.851143\n",
      "Train Epoch: 10 [17600/50000 (35%)]\tLoss: 1.836151\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.838974\n",
      "Train Epoch: 10 [20800/50000 (42%)]\tLoss: 1.857766\n",
      "Train Epoch: 10 [22400/50000 (45%)]\tLoss: 1.961073\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.701059\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.835610\n",
      "Train Epoch: 10 [27200/50000 (54%)]\tLoss: 1.959889\n",
      "Train Epoch: 10 [28800/50000 (58%)]\tLoss: 1.898639\n",
      "Train Epoch: 10 [30400/50000 (61%)]\tLoss: 1.921964\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.773686\n",
      "Train Epoch: 10 [33600/50000 (67%)]\tLoss: 1.898650\n",
      "Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [36800/50000 (74%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.755188\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.711054\n",
      "Train Epoch: 10 [41600/50000 (83%)]\tLoss: 1.647485\n",
      "Train Epoch: 10 [43200/50000 (86%)]\tLoss: 1.646782\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.825516\n",
      "Train Epoch: 10 [46400/50000 (93%)]\tLoss: 1.960655\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.772429\n",
      "Train Epoch: 10 [49600/50000 (99%)]\tLoss: 1.709450\n",
      "Accuracy......\n",
      "\n",
      "Training_set: Average loss: 0.1109, Accuracy: 34338/50000 (69%)\n",
      "\n",
      "Accuracy......\n",
      "\n",
      "Test_Set: Average loss: 0.1124, Accuracy: 6619/10000 (66%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.927757\n",
      "Train Epoch: 10 [1600/50000 (3%)]\tLoss: 1.836151\n",
      "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 1.586151\n",
      "Train Epoch: 10 [4800/50000 (10%)]\tLoss: 1.957122\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.711136\n",
      "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 2.148651\n",
      "Train Epoch: 10 [9600/50000 (19%)]\tLoss: 1.961164\n",
      "Train Epoch: 10 [11200/50000 (22%)]\tLoss: 1.711151\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.711149\n",
      "Train Epoch: 10 [14400/50000 (29%)]\tLoss: 2.023081\n",
      "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.711151\n",
      "Train Epoch: 10 [17600/50000 (35%)]\tLoss: 1.773348\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [20800/50000 (42%)]\tLoss: 1.773675\n",
      "Train Epoch: 10 [22400/50000 (45%)]\tLoss: 2.023445\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.961533\n",
      "Train Epoch: 10 [27200/50000 (54%)]\tLoss: 1.711112\n",
      "Train Epoch: 10 [28800/50000 (58%)]\tLoss: 1.586150\n",
      "Train Epoch: 10 [30400/50000 (61%)]\tLoss: 1.605086\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.621835\n",
      "Train Epoch: 10 [33600/50000 (67%)]\tLoss: 1.835932\n",
      "Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.960904\n",
      "Train Epoch: 10 [36800/50000 (74%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.836152\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.898220\n",
      "Train Epoch: 10 [41600/50000 (83%)]\tLoss: 1.773642\n",
      "Train Epoch: 10 [43200/50000 (86%)]\tLoss: 1.833871\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.648634\n",
      "Train Epoch: 10 [46400/50000 (93%)]\tLoss: 1.714145\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.898651\n",
      "Train Epoch: 10 [49600/50000 (99%)]\tLoss: 1.773650\n",
      "Accuracy......\n",
      "\n",
      "Training_set: Average loss: 0.1139, Accuracy: 31899/50000 (64%)\n",
      "\n",
      "Accuracy......\n",
      "\n",
      "Test_Set: Average loss: 0.1153, Accuracy: 6163/10000 (62%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.711249\n",
      "Train Epoch: 10 [1600/50000 (3%)]\tLoss: 1.772424\n",
      "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 1.897706\n",
      "Train Epoch: 10 [4800/50000 (10%)]\tLoss: 1.711151\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.896080\n",
      "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.821409\n",
      "Train Epoch: 10 [9600/50000 (19%)]\tLoss: 1.898650\n",
      "Train Epoch: 10 [11200/50000 (22%)]\tLoss: 1.709108\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.773650\n",
      "Train Epoch: 10 [14400/50000 (29%)]\tLoss: 1.958959\n",
      "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.960933\n",
      "Train Epoch: 10 [17600/50000 (35%)]\tLoss: 1.888608\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.921417\n",
      "Train Epoch: 10 [20800/50000 (42%)]\tLoss: 1.849949\n",
      "Train Epoch: 10 [22400/50000 (45%)]\tLoss: 1.960561\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.841493\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 2.023534\n",
      "Train Epoch: 10 [27200/50000 (54%)]\tLoss: 1.961151\n",
      "Train Epoch: 10 [28800/50000 (58%)]\tLoss: 2.050821\n",
      "Train Epoch: 10 [30400/50000 (61%)]\tLoss: 1.648652\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.817770\n",
      "Train Epoch: 10 [33600/50000 (67%)]\tLoss: 1.709432\n",
      "Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.773651\n",
      "Train Epoch: 10 [36800/50000 (74%)]\tLoss: 1.766448\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.835313\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.643756\n",
      "Train Epoch: 10 [41600/50000 (83%)]\tLoss: 1.775782\n",
      "Train Epoch: 10 [43200/50000 (86%)]\tLoss: 1.711151\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.960533\n",
      "Train Epoch: 10 [46400/50000 (93%)]\tLoss: 1.711171\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.773582\n",
      "Train Epoch: 10 [49600/50000 (99%)]\tLoss: 1.711150\n",
      "Accuracy......\n",
      "\n",
      "Training_set: Average loss: 0.1131, Accuracy: 32521/50000 (65%)\n",
      "\n",
      "Accuracy......\n",
      "\n",
      "Test_Set: Average loss: 0.1137, Accuracy: 6424/10000 (64%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.773811\n",
      "Train Epoch: 10 [1600/50000 (3%)]\tLoss: 1.773473\n",
      "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 2.023623\n",
      "Train Epoch: 10 [4800/50000 (10%)]\tLoss: 1.833902\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.961151\n",
      "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.833718\n",
      "Train Epoch: 10 [9600/50000 (19%)]\tLoss: 1.960773\n",
      "Train Epoch: 10 [11200/50000 (22%)]\tLoss: 2.086149\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.961204\n",
      "Train Epoch: 10 [14400/50000 (29%)]\tLoss: 2.023175\n",
      "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.834023\n",
      "Train Epoch: 10 [17600/50000 (35%)]\tLoss: 1.898964\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.898651\n",
      "Train Epoch: 10 [20800/50000 (42%)]\tLoss: 1.840722\n",
      "Train Epoch: 10 [22400/50000 (45%)]\tLoss: 1.836151\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.836157\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 2.086137\n",
      "Train Epoch: 10 [27200/50000 (54%)]\tLoss: 1.772541\n",
      "Train Epoch: 10 [28800/50000 (58%)]\tLoss: 1.836151\n",
      "Train Epoch: 10 [30400/50000 (61%)]\tLoss: 2.023651\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.898648\n",
      "Train Epoch: 10 [33600/50000 (67%)]\tLoss: 1.836150\n",
      "Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.963046\n",
      "Train Epoch: 10 [36800/50000 (74%)]\tLoss: 1.589463\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.712610\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.836151\n",
      "Train Epoch: 10 [41600/50000 (83%)]\tLoss: 1.586146\n",
      "Train Epoch: 10 [43200/50000 (86%)]\tLoss: 1.782066\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.711150\n",
      "Train Epoch: 10 [46400/50000 (93%)]\tLoss: 1.960769\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.711150\n",
      "Train Epoch: 10 [49600/50000 (99%)]\tLoss: 1.586158\n",
      "Accuracy......\n",
      "\n",
      "Training_set: Average loss: 0.1139, Accuracy: 31896/50000 (64%)\n",
      "\n",
      "Accuracy......\n",
      "\n",
      "Test_Set: Average loss: 0.1151, Accuracy: 6199/10000 (62%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.898651\n",
      "Train Epoch: 10 [1600/50000 (3%)]\tLoss: 1.591243\n",
      "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 1.895956\n",
      "Train Epoch: 10 [4800/50000 (10%)]\tLoss: 1.836150\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 1.711150\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_train,loss_test,acc_test,acc_train,ep = model_activate(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "pkl.dump(acc_test,open(\"acc_test_(3).pkl\",\"wb\"))\n",
    "pkl.dump(acc_train,open(\"acc_train_(3).pkl\",\"wb\"))\n",
    "pkl.dump(ep,open(\"ep.pkl_full_train_(3).pkl.\",\"wb\"))\n",
    "pkl.dump(loss_test,open(\"loss_test_(3).pkl.\",\"wb\"))\n",
    "pkl.dump(loss_train,open(\"loss_train_(3).pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_plot(ep,acc_train,acc_test,\"Acc_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
